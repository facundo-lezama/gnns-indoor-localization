{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vp0qGc5z4AOk",
    "tags": []
   },
   "source": [
    "# MNAV - Heterogeneous GNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oDifkXbM4UUX"
   },
   "source": [
    "Dataset: MNAV\n",
    "\n",
    "Modelo: GNN con grafo heterogéneo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n4qV8TIQYl62",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Importar Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 27989,
     "status": "ok",
     "timestamp": 1635510361213,
     "user": {
      "displayName": "Facundo Lezama",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjvX7z6H8HKQ9Q4HSRF20NaPjvwtJ3pQh0TGed2XQ=s64",
      "userId": "08104556030349101598"
     },
     "user_tz": 180
    },
    "id": "OjHf3j75Agti"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from itertools import combinations\n",
    "from copy import deepcopy\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "import torch\n",
    "import networkx as nx\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "from torch_geometric.data import Data, HeteroData\n",
    "from torch_geometric.nn.conv.dna_conv import Linear\n",
    "from torch_geometric.utils import to_networkx, is_undirected, to_undirected\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, ChebConv, SAGEConv, TAGConv, GraphConv, to_hetero, GATConv, Linear, BatchNorm, HeteroConv, HANConv\n",
    "from torch_geometric.loader import DataLoader\n",
    "import torch_geometric.transforms as T\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cMFznhF9wDxB"
   },
   "source": [
    "El dataset se encuentra disponible en https://github.com/ffedee7/posifi_mnav/tree/master/data_analysis. El dataset disponible está anonimizado pero se puede deshacer con el código de ese repositorio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1270,
     "status": "ok",
     "timestamp": 1635454213295,
     "user": {
      "displayName": "Facundo Lezama",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjvX7z6H8HKQ9Q4HSRF20NaPjvwtJ3pQh0TGed2XQ=s64",
      "userId": "08104556030349101598"
     },
     "user_tz": 180
    },
    "id": "jXw5-YfeAgtp"
   },
   "outputs": [],
   "source": [
    "dataset = 'datos2.csv'\n",
    "df = pd.read_csv(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 383
    },
    "executionInfo": {
     "elapsed": 1105,
     "status": "ok",
     "timestamp": 1635454214395,
     "user": {
      "displayName": "Facundo Lezama",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjvX7z6H8HKQ9Q4HSRF20NaPjvwtJ3pQh0TGed2XQ=s64",
      "userId": "08104556030349101598"
     },
     "user_tz": 180
    },
    "id": "OIhn7QSoAgtq",
    "outputId": "37bf3f47-b29c-4af5-aec4-a23a675b0eef"
   },
   "outputs": [],
   "source": [
    "print(df.shape)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1635454214396,
     "user": {
      "displayName": "Facundo Lezama",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjvX7z6H8HKQ9Q4HSRF20NaPjvwtJ3pQh0TGed2XQ=s64",
      "userId": "08104556030349101598"
     },
     "user_tz": 180
    },
    "id": "hDAHd9mXpwec",
    "outputId": "dbf5f05a-eb40-47e6-c075-1e7fd39f104f"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gfqVFAK32Th4",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Preprocesamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CL6O8p3ZERrS"
   },
   "source": [
    "Se definen los APs que se quieren usar tanto para la construcción del grafo como para el modelo.\n",
    "\n",
    "En este caso usamos solamente los APs que se encuentran dentro del MNAV, es decir que descartamos los APs que aparecen en las medidas pero que no son los que se instalaron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1635454214396,
     "user": {
      "displayName": "Facundo Lezama",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjvX7z6H8HKQ9Q4HSRF20NaPjvwtJ3pQh0TGed2XQ=s64",
      "userId": "08104556030349101598"
     },
     "user_tz": 180
    },
    "id": "ynWQ8U8z50to"
   },
   "outputs": [],
   "source": [
    "APs_MAC_2_4 = ['wifi-dc:a5:f4:43:85:c0',\n",
    "'wifi-dc:a5:f4:43:27:e0',\n",
    "'wifi-f8:4f:57:ab:da:00',\n",
    "'wifi-5c:a4:8a:4c:05:c0',\n",
    "'wifi-1c:1d:86:ce:ef:b0',\n",
    "'wifi-dc:a5:f4:43:79:20',\n",
    "'wifi-c0:7b:bc:36:9e:10',\n",
    "'wifi-1c:1d:86:9f:99:20',\n",
    "'wifi-c0:7b:bc:36:af:40',\n",
    "'wifi-c0:7b:bc:36:af:80',\n",
    "'wifi-1c:1d:86:b6:ac:80',\n",
    "'wifi-dc:a5:f4:43:72:e0',\n",
    "'wifi-f8:4f:57:ab:d8:60',\n",
    "'wifi-dc:a5:f4:43:72:90',\n",
    "'wifi-f8:4f:57:ab:ce:20']\n",
    "\n",
    "APs_MAC_5 = ['wifi-dc:a5:f4:45:85:b0',\n",
    "'wifi-dc:a5:f4:45:27:e0',\n",
    "'wifi-f8:4f:57:ad:d9:60',\n",
    "'wifi-5c:a4:8a:4e:05:30',\n",
    "'wifi-1c:1d:86:d0:ef:00',\n",
    "'wifi-dc:a5:f4:45:79:10',\n",
    "'wifi-c0:7b:bc:38:9e:00',\n",
    "'wifi-1c:1d:86:a1:99:00',\n",
    "'wifi-c0:7b:bc:38:af:30',\n",
    "'wifi-c0:7b:bc:38:af:70',\n",
    "'wifi-1c:1d:86:b8:ac:80',\n",
    "'wifi-dc:a5:f4:45:72:d0',\n",
    "'wifi-f8:4f:57:ad:d7:c0',\n",
    "'wifi-dc:a5:f4:45:72:80',\n",
    "'wifi-f8:4f:57:ad:cd:80']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1635454214397,
     "user": {
      "displayName": "Facundo Lezama",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjvX7z6H8HKQ9Q4HSRF20NaPjvwtJ3pQh0TGed2XQ=s64",
      "userId": "08104556030349101598"
     },
     "user_tz": 180
    },
    "id": "q0BCpSLFAgts"
   },
   "outputs": [],
   "source": [
    "def preprocess_dataset(dataset_path, zone_to_remove=None, dataset_percentage=None):\n",
    "    df = pd.read_csv(dataset_path)\n",
    "    # paso los NaN a 0\n",
    "    df = df.fillna(0) \n",
    "\n",
    "    # sumo 100 a los valores de RSSI y ahora 0 es el minimo\n",
    "    df.iloc[:,1:] = 100 + df.iloc[:,1:] \n",
    "    values = df.iloc[:,1:]\n",
    "\n",
    "    # las medidas originales en 0 las asumo como que estaban muy lejos\n",
    "    # entonces las dejo en 0 que es el nuevo valor minimo\n",
    "    values[values==100] = 0 \n",
    "    df.iloc[:,1:] = values    \n",
    "    \n",
    "    # armo dos datsets: uno con las medidas solamente de la frecuencia\n",
    "    # 2.4GHz y otro con las frecuencias 2.4GHz y 5GHz\n",
    "    data_2_4 = df[['location'] + APs_MAC_2_4] # REVISAR PORQUE CREO QUE NO LO VUELVO A USAR\n",
    "    data_2_4_5 = df[['location'] + APs_MAC_2_4 + APs_MAC_5]\n",
    "    \n",
    "    if dataset_percentage:\n",
    "        data_2_4_5 = data_2_4_5.sample(frac=dataset_percentage)\n",
    "    \n",
    "    # paso las zonas por un ordinal encoder\n",
    "    enc = OrdinalEncoder(dtype=int)\n",
    "    y = enc.fit_transform(data_2_4_5['location'].values.reshape(-1,1))\n",
    "    X = data_2_4_5.iloc[:,1:].values\n",
    "\n",
    "    print(enc.categories_)\n",
    "\n",
    "    # separo el dataset en train y test 80-20\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)    \n",
    "    \n",
    "#     if zone_to_remove:\n",
    "#         X_train_modif = X_train[(y_train != zone_to_remove).reshape(-1)]\n",
    "#         y_train_modif = y_train[(y_train != zone_to_remove).reshape(-1)]\n",
    "#         y_train_modif[y_train_modif>zone_to_remove] = y_train_modif[y_train_modif>zone_to_remove] - 1\n",
    "        \n",
    "#         X_test_modif = X_test[(y_test != zone_to_remove).reshape(-1)]\n",
    "#         y_test_modif = y_test[(y_test != zone_to_remove).reshape(-1)]\n",
    "#         y_test_modif[y_test_modif>zone_to_remove] = y_test_modif[y_test_modif>zone_to_remove] - 1\n",
    "        \n",
    "#         print(\"X_train shape: \", X_train_modif.shape)\n",
    "#         return X_train_modif, X_test_modif, y_train_modif, y_test_modif, len(APs_MAC_2_4), len(np.unique(y_train_modif)), enc\n",
    "        \n",
    "    print(\"X_train shape: \", X_train.shape)\n",
    "    return X_train, X_test, y_train, y_test, len(APs_MAC_2_4), len(np.unique(y_train)), enc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fmC2Dsazdx7i",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Grafo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cF2ZViCq3a_Q"
   },
   "source": [
    "En las siguientes celdas se describe un poco el dataset y se muestran las distribuciones de potencia por AP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1635454218455,
     "user": {
      "displayName": "Facundo Lezama",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjvX7z6H8HKQ9Q4HSRF20NaPjvwtJ3pQh0TGed2XQ=s64",
      "userId": "08104556030349101598"
     },
     "user_tz": 180
    },
    "id": "xHfjTzU5E2eq"
   },
   "outputs": [],
   "source": [
    "def ap_graph_creator(X_G, th=10, cols=None, prune_th=0):\n",
    "    \"\"\"\n",
    "    Dado un dataset y un threshold se arma un grafo basado en las medidas de RRSI\n",
    "    \"\"\"\n",
    "    columns = cols if cols else ['AP1', 'AP2', 'AP3', 'AP4', 'AP5', 'AP6', 'AP7', 'AP8', 'AP9', 'AP10', 'AP11', 'AP12', 'AP13', 'AP14', 'AP15']\n",
    "    df_data_train = pd.DataFrame(X_G, columns=columns)\n",
    "    df_G = pd.DataFrame(columns = ['from', 'to', 'weight'])\n",
    "\n",
    "    for ap in columns:\n",
    "        # para cada AP me quedo con las instancias donde el RSSI esta en el rango\n",
    "        # (max-th) intentando estimar las instancias mas cercanas al AP\n",
    "        max_val = df_data_train[ap].max()\n",
    "        # print(f\"Max: {max_val}\")\n",
    "        df_aux_i = df_data_train[df_data_train[ap]  > (max_val - th)]\n",
    "        df_aux_i = df_aux_i.drop(ap, axis=1)\n",
    "        # print(f\"Counts: {df_aux_i.shape[0]}\")\n",
    "        # import ipdb; ipdb.set_trace()\n",
    "        for k, v in df_aux_i.mean().items():\n",
    "            # print(v)\n",
    "            # armo las aristas con el promedio de RSSI que ven las instancias\n",
    "            # filtradas al resto de los APs\n",
    "            # weight = v\n",
    "            # if df_G.loc[(df_G['from'] == k) & (df_G['to'] == ap)].weight.any():\n",
    "            #     weight = np.mean([float(df_G.loc[(df_G['from'] == k) & (df_G['to'] == ap)].weight), weight])\n",
    "            #     df_G.loc[(df_G['from'] == k) & (df_G['to'] == ap)] = k, ap, weight\n",
    "            if v > prune_th:\n",
    "                df_G = df_G.append({'from':ap, 'to': k, 'weight': v}, ignore_index=True)\n",
    "        \n",
    "\n",
    "    edge_index_first_row = []\n",
    "    edge_index_second_row = []\n",
    "    edge_attr = []\n",
    "    for index, row in df_G.iterrows():\n",
    "        edge_index_first_row.append(columns.index(row['from']))\n",
    "        edge_index_second_row.append(columns.index(row['to']))\n",
    "        edge_attr.append([float(row.weight)])\n",
    "\n",
    "    edge_index = torch.tensor([edge_index_first_row, edge_index_second_row], dtype=torch.long)\n",
    "    edge_attr = torch.tensor(edge_attr, dtype=torch.float)                           \n",
    "    edge_index, edge_attr = to_undirected(edge_index, edge_attr, reduce=\"mean\")\n",
    "\n",
    "    return edge_index, edge_attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1635454218455,
     "user": {
      "displayName": "Facundo Lezama",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjvX7z6H8HKQ9Q4HSRF20NaPjvwtJ3pQh0TGed2XQ=s64",
      "userId": "08104556030349101598"
     },
     "user_tz": 180
    },
    "id": "xHfjTzU5E2eq"
   },
   "outputs": [],
   "source": [
    "def zone_ap_graph_creator(X_G, y, cols=None, prune_th=0, zone_to_remove=None):\n",
    "\n",
    "    columns = cols if cols else ['AP1', 'AP2', 'AP3', 'AP4', 'AP5', 'AP6', 'AP7', 'AP8', 'AP9', 'AP10', 'AP11', 'AP12', 'AP13', 'AP14', 'AP15']\n",
    "    df_data_train = pd.DataFrame(X_G, columns=columns)\n",
    "    df_data_train['cls'] = y\n",
    "    df_G = pd.DataFrame(columns = ['from', 'to', 'weight'])\n",
    "\n",
    "    for zone in range(len(df_data_train['cls'].unique())): # np.sort(df_data_train['cls'].unique()):\n",
    "\n",
    "        filtered_instances = df_data_train.loc[df_data_train['cls'] == zone]\n",
    "        means = np.array(filtered_instances.mean())[:-1]\n",
    "\n",
    "        for ap, mean in enumerate(means):\n",
    "            if mean > prune_th:\n",
    "                # if zone_to_remove != zone:\n",
    "                if zone_to_remove == zone:\n",
    "                    mean = 0\n",
    "                df_G = df_G.append({'from':zone, 'to': ap, 'weight': mean}, ignore_index=True)\n",
    "\n",
    "    edge_index_first_row = []\n",
    "    edge_index_second_row = []\n",
    "    edge_attr = []\n",
    "    for index, row in df_G.iterrows():\n",
    "        edge_index_first_row.append(row['from'])\n",
    "        edge_index_second_row.append(row['to'])\n",
    "        edge_attr.append([float(row.weight)])\n",
    "    \n",
    "    edge_index = torch.tensor([[int(i) for i in edge_index_first_row], [int(i) for i in edge_index_second_row]], dtype=torch.long)\n",
    "    edge_attr = torch.tensor(edge_attr, dtype=torch.float)                           \n",
    "\n",
    "    return edge_index, edge_attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(X, y, graph, zone_to_remove=None):\n",
    "    dataset = []\n",
    "    # random = torch.randn(16,5)\n",
    "    one_hot = torch.nn.functional.one_hot(torch.arange(0,16)).float()\n",
    "    \n",
    "    for i in range(len(y)):\n",
    "        if y[i] != zone_to_remove:\n",
    "            data = deepcopy(graph)\n",
    "            data['aps'].x = torch.Tensor(X[i])\n",
    "\n",
    "            # data['zones'].x = torch.ones(data['zones'].num_nodes,1)\n",
    "            data['zones'].x = torch.zeros(data['zones'].num_nodes,1)\n",
    "            # data['zones'].x = torch.randn(data['zones'].num_nodes,5)\n",
    "            # data['zones'].x = one_hot\n",
    "            # data['zones'].x -= data['zones'].x.mean()\n",
    "            # data['zones'].x /= data['zones'].x.std()\n",
    "            data['zones'].y = torch.Tensor(y[i])\n",
    "            data['zones'].train_mask = torch.Tensor([True]*len(y))\n",
    "            data['zones'].val_mask = torch.Tensor([True]*len(y))\n",
    "            data['zones'].test_mask = torch.Tensor([True]*len(y))\n",
    "\n",
    "            dataset.append(data)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_heterodata(ap_edge_index, ap_edge_attr, zone_ap_edge_index, zone_ap_edge_attr, classes=16):\n",
    "    data = HeteroData()\n",
    "    data['aps', 'ap_ap', 'aps'].edge_index = ap_edge_index\n",
    "    ap_edge_attr = (ap_edge_attr - ap_edge_attr.mean())/ap_edge_attr.std()\n",
    "    data['aps', 'ap_ap', 'aps'].edge_attr = ap_edge_attr\n",
    "    data['aps'].num_nodes = 15\n",
    "\n",
    "    data['zones', 'zone_ap', 'aps'].edge_index = zone_ap_edge_index\n",
    "    zone_ap_edge_attr = (zone_ap_edge_attr - zone_ap_edge_attr.mean())/zone_ap_edge_attr.std()    \n",
    "    data['zones', 'zone_ap', 'aps'].edge_attr = torch.nn.functional.normalize(zone_ap_edge_attr, dim=0)\n",
    "    data['zones'].num_nodes = classes\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeteroGNN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, out_channels, hidden_layers):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hidden_convs = torch.nn.ModuleList()\n",
    "        for _ in range(hidden_layers):\n",
    "            conv = HeteroConv({\n",
    "                ('aps', 'ap_ap', 'aps'): GraphConv((-1, -1), hidden_channels),\n",
    "                ('zones', 'zone_ap', 'aps'): GraphConv((-1, -1), hidden_channels),\n",
    "                ('aps', 'rev_zone_ap', 'zones'): GraphConv((-1, -1), hidden_channels),\n",
    "            }, aggr='mean')\n",
    "            self.hidden_convs.append(conv)\n",
    "        \n",
    "        self.out_convs = torch.nn.ModuleList()\n",
    "        conv = HeteroConv({\n",
    "            ('aps', 'ap_ap', 'aps'): GraphConv((-1, -1), out_channels),\n",
    "            ('zones', 'zone_ap', 'aps'): GraphConv((-1, -1), out_channels),\n",
    "            ('aps', 'rev_zone_ap', 'zones'): GraphConv((-1, -1), out_channels),\n",
    "        }, aggr='mean')\n",
    "        self.out_convs.append(conv)        \n",
    "\n",
    "        # self.lin = Linear(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x_dict, edge_index_dict, edge_attr_dict):\n",
    "        for conv in self.hidden_convs:\n",
    "            x_dict = conv(x_dict, edge_index_dict, edge_attr_dict)\n",
    "            x_dict = {key: x.relu() for key, x in x_dict.items()}\n",
    "        for conv in self.out_convs:\n",
    "            x_dict = conv(x_dict, edge_index_dict, edge_attr_dict)\n",
    "           # x_dict = {key: x.relu() for key, x in x_dict.items()}\n",
    "\n",
    "        # out = self.lin(x_dict['zones'])\n",
    "        \n",
    "        return x_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeteroGNN_simplified(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, out_channels, hidden_layers):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hidden_convs = torch.nn.ModuleList()\n",
    "        for _ in range(hidden_layers):\n",
    "            conv = HeteroConv({\n",
    "                ('aps', 'ap_ap', 'aps'): GraphConv((-1, -1), hidden_channels),\n",
    "                # ('zones', 'zone_ap', 'aps'): GraphConv((-1, -1), hidden_channels),\n",
    "                # ('aps', 'rev_zone_ap', 'zones'): GraphConv((-1, -1), hidden_channels),\n",
    "            }, aggr='sum')\n",
    "            self.hidden_convs.append(conv)\n",
    "        \n",
    "        self.out_convs = torch.nn.ModuleList()\n",
    "        conv = HeteroConv({\n",
    "            ('aps', 'ap_ap', 'aps'): GraphConv((-1, -1), out_channels),\n",
    "            ('zones', 'zone_ap', 'aps'): GraphConv((-1, -1), out_channels),\n",
    "            ('aps', 'rev_zone_ap', 'zones'): GraphConv((-1, -1), out_channels),\n",
    "        }, aggr='sum')\n",
    "        self.out_convs.append(conv)        \n",
    "\n",
    "        # self.lin = Linear(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x_dict, edge_index_dict, edge_attr_dict):\n",
    "        initial_zones_signal = x_dict[\"zones\"]\n",
    "        for conv in self.hidden_convs:\n",
    "            x_dict = conv(x_dict, edge_index_dict, edge_attr_dict)\n",
    "            x_dict = {key: x.relu() for key, x in x_dict.items()}\n",
    "        x_dict[\"zones\"] = initial_zones_signal\n",
    "        for conv in self.out_convs:\n",
    "            x_dict = conv(x_dict, edge_index_dict, edge_attr_dict)\n",
    "           # x_dict = {key: x.relu() for key, x in x_dict.items()}\n",
    "\n",
    "        # out = self.lin(x_dict['zones'])\n",
    "        \n",
    "        return x_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YHsZyzkAZKlK",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Armado dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 356,
     "status": "ok",
     "timestamp": 1635462144424,
     "user": {
      "displayName": "Facundo Lezama",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjvX7z6H8HKQ9Q4HSRF20NaPjvwtJ3pQh0TGed2XQ=s64",
      "userId": "08104556030349101598"
     },
     "user_tz": 180
    },
    "id": "0MsSCtZkUDbj"
   },
   "outputs": [],
   "source": [
    "# Split de los datos y armado del objeto Data con el grafo\n",
    "\n",
    "X_train, X_test, y_train, y_test, num_aps, num_classes, enc_train = preprocess_dataset(dataset, dataset_percentage=0.3)\n",
    "ap_edge_index, ap_edge_attr = ap_graph_creator(X_train[:,:15], th=20, prune_th=20) #el grafo lo armo solo con los datos de 2.4Ghz\n",
    "zone_ap_edge_index, zone_ap_edge_attr = zone_ap_graph_creator(X_train[:,:15], y_train, prune_th=20) #el grafo lo armo solo con los datos de 2.4Ghz\n",
    "data = build_heterodata(ap_edge_index, ap_edge_attr, zone_ap_edge_index, zone_ap_edge_attr)\n",
    "T.ToUndirected()(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Armado del dataset\n",
    "\n",
    "x_training_data = np.reshape(X_train,(X_train.shape[0],15,2))\n",
    "x_test_data = np.reshape(X_test,(X_test.shape[0],15,2))\n",
    "y_training_data = y_train\n",
    "y_test_data = y_test\n",
    "\n",
    "#normalize (x-mean)/std\n",
    "mean = x_training_data.mean(axis=0)\n",
    "std = x_training_data.std(axis=0)\n",
    "\n",
    "x_training_data = x_training_data - mean\n",
    "x_training_data /= std\n",
    "x_test_data = x_test_data - mean\n",
    "x_test_data /= std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = build_dataset(x_training_data, y_training_data, data)\n",
    "test_dataset = build_dataset(x_test_data, y_test_data, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device('cuda:1')\n",
    "# device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### HeteroGNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HeteroGNN(20, 1, 4).to(device)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=len(test_dataset), shuffle=False)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=5e-4)\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.8)\n",
    "print_every = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with torch.no_grad():  # Initialize lazy modules.\n",
    "#     batch = train_dataset[0].to(device)\n",
    "#     out = model(batch.x_dict, batch.edge_index_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = []\n",
    "train_accuracy = []\n",
    "test_loss = []\n",
    "test_accuracy = []\n",
    "best_test_accuracy = 0\n",
    "\n",
    "\n",
    "m = torch.nn.Softmax(dim=1)\n",
    "\n",
    "for epoch in range(250):\n",
    "    # print(f\"Epoch: {epoch+1}\")\n",
    "    \n",
    "    # TRAIN\n",
    "    model.train()\n",
    "    train_accuracy_epoch = []\n",
    "    train_loss_epoch = []\n",
    "    for data in train_loader:\n",
    "        \n",
    "        data = data.to(device)\n",
    "        out = model(data.x_dict, data.edge_index_dict, data.edge_attr_dict) \n",
    "        out_zones = out[\"zones\"].cpu().reshape(out[\"zones\"].cpu().shape[0]//num_classes,num_classes)\n",
    "\n",
    "        loss_result = loss(out_zones.cpu(), data[\"zones\"].y.cpu().type(torch.long))\n",
    "        loss_result.backward()\n",
    "        train_loss_epoch.append(loss_result.detach().cpu())\n",
    "        \n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = m(out_zones)\n",
    "        train_accuracy_epoch.append(accuracy_score(data[\"zones\"].y.cpu().reshape(-1).type(torch.long), np.array(torch.argmax(output.cpu(), axis=1))))\n",
    "\n",
    "    # if scheduler.get_last_lr()[0] > 0.0005:\n",
    "    if (epoch+1)%20 == 0:\n",
    "        scheduler.step()\n",
    "\n",
    "    train_accuracy.append(np.mean(train_accuracy_epoch))\n",
    "    train_loss.append(np.mean(train_loss_epoch))\n",
    "    \n",
    "    \n",
    "    \n",
    "    # VALIDATION\n",
    "    model.eval()\n",
    "    test_accuracy_epoch = []\n",
    "    test_loss_epoch = []\n",
    "    for data in test_loader:\n",
    "        \n",
    "        data = data.to(device)\n",
    "        out = model(data.x_dict, data.edge_index_dict, data.edge_attr_dict) \n",
    "        # out_zones = torch.mean(out[\"zones\"].cpu(),1)\n",
    "        # out_zones = out_zones.reshape(out_zones.shape[0]//num_classes,num_classes)\n",
    "        out_zones = out[\"zones\"].cpu().reshape(out[\"zones\"].cpu().shape[0]//num_classes,num_classes)\n",
    "\n",
    "        # out_zones = out.cpu().reshape(out.cpu().shape[0]//num_classes,num_classes)\n",
    "        \n",
    "        loss_result = loss(out_zones.cpu(), data[\"zones\"].y.cpu().type(torch.long))        \n",
    "        test_loss_epoch.append(loss_result.detach().cpu())\n",
    "        \n",
    "        output = m(out_zones)\n",
    "        test_accuracy_epoch.append(accuracy_score(data[\"zones\"].y.cpu().reshape(-1).type(torch.long), np.array(torch.argmax(output.cpu(), axis=1))))\n",
    "\n",
    "    test_accuracy.append(np.mean(test_accuracy_epoch))\n",
    "    if test_accuracy[-1] > best_test_accuracy:\n",
    "        best_test_accuracy = test_accuracy[-1]        \n",
    "        torch.save(model.state_dict(), \"MNAV_HeteroGNN_best_model.pth\")\n",
    "        \n",
    "    test_loss.append(np.mean(test_loss_epoch))\n",
    "\n",
    "    if (epoch+1)%print_every == 0:\n",
    "        print(f\"Epoch {epoch+1}, Train Loss {np.mean(train_loss_epoch)}, Val Loss {np.mean(test_loss_epoch)}\")\n",
    "    \n",
    "print(f\"Last LR: {scheduler.get_last_lr()}\")\n",
    "print(f\"Best Accuracy: Train {np.max(train_accuracy)}, Val {np.max(test_accuracy)}\")\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(train_loss, label=\"Train loss\")\n",
    "plt.plot(test_loss, label=\"Validation loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(train_accuracy, label=\"Train accuracy\")\n",
    "plt.plot(test_accuracy, label=\"Validation accuracy\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = torch.nn.Softmax(dim=1)\n",
    "output = m(out_zones)\n",
    "accuracy = accuracy_score(data[\"zones\"].y.cpu().reshape(-1).type(torch.long), np.array(torch.argmax(output.cpu(), axis=1)))\n",
    "\n",
    "print(accuracy)\n",
    "print(classification_report(data[\"zones\"].y.cpu().reshape(-1).type(torch.long), np.array(torch.argmax(output.cpu(), axis=1))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### HeteroGNNSimplified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HeteroGNN_simplified(20, 1, 4).to(device)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=len(test_dataset), shuffle=False)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=5e-4)\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.8)\n",
    "print_every = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = []\n",
    "train_accuracy = []\n",
    "test_loss = []\n",
    "test_accuracy = []\n",
    "best_test_accuracy = 0\n",
    "\n",
    "\n",
    "m = torch.nn.Softmax(dim=1)\n",
    "\n",
    "for epoch in range(250):\n",
    "    # print(f\"Epoch: {epoch+1}\")\n",
    "    \n",
    "    # TRAIN\n",
    "    model.train()\n",
    "    train_accuracy_epoch = []\n",
    "    train_loss_epoch = []\n",
    "    for data in train_loader:\n",
    "        \n",
    "        data = data.to(device)\n",
    "        out = model(data.x_dict, data.edge_index_dict, data.edge_attr_dict) \n",
    "        out_zones = out[\"zones\"].cpu().reshape(out[\"zones\"].cpu().shape[0]//num_classes,num_classes)\n",
    "\n",
    "        loss_result = loss(out_zones.cpu(), data[\"zones\"].y.cpu().type(torch.long))\n",
    "        loss_result.backward()\n",
    "        train_loss_epoch.append(loss_result.detach().cpu())\n",
    "        \n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = m(out_zones)\n",
    "        train_accuracy_epoch.append(accuracy_score(data[\"zones\"].y.cpu().reshape(-1).type(torch.long), np.array(torch.argmax(output.cpu(), axis=1))))\n",
    "\n",
    "    # if scheduler.get_last_lr()[0] > 0.0005:\n",
    "    if (epoch+1)%40 == 0:\n",
    "        scheduler.step()\n",
    "\n",
    "    train_accuracy.append(np.mean(train_accuracy_epoch))\n",
    "    train_loss.append(np.mean(train_loss_epoch))\n",
    "    \n",
    "    \n",
    "    \n",
    "    # VALIDATION\n",
    "    model.eval()\n",
    "    test_accuracy_epoch = []\n",
    "    test_loss_epoch = []\n",
    "    for data in test_loader:\n",
    "        \n",
    "        data = data.to(device)\n",
    "        out = model(data.x_dict, data.edge_index_dict, data.edge_attr_dict) \n",
    "        # out_zones = torch.mean(out[\"zones\"].cpu(),1)\n",
    "        # out_zones = out_zones.reshape(out_zones.shape[0]//num_classes,num_classes)\n",
    "        out_zones = out[\"zones\"].cpu().reshape(out[\"zones\"].cpu().shape[0]//num_classes,num_classes)\n",
    "\n",
    "        # out_zones = out.cpu().reshape(out.cpu().shape[0]//num_classes,num_classes)\n",
    "        \n",
    "        loss_result = loss(out_zones.cpu(), data[\"zones\"].y.cpu().type(torch.long))        \n",
    "        test_loss_epoch.append(loss_result.detach().cpu())\n",
    "        \n",
    "        output = m(out_zones)\n",
    "        test_accuracy_epoch.append(accuracy_score(data[\"zones\"].y.cpu().reshape(-1).type(torch.long), np.array(torch.argmax(output.cpu(), axis=1))))\n",
    "\n",
    "    test_accuracy.append(np.mean(test_accuracy_epoch))\n",
    "    if test_accuracy[-1] > best_test_accuracy:\n",
    "        best_test_accuracy = test_accuracy[-1]        \n",
    "        torch.save(model.state_dict(), \"MNAV_HeteroGNN_best_model.pth\")\n",
    "        \n",
    "    test_loss.append(np.mean(test_loss_epoch))\n",
    "\n",
    "    if (epoch+1)%print_every == 0:\n",
    "        print(f\"Epoch {epoch+1}, Train Loss {np.mean(train_loss_epoch)}, Val Loss {np.mean(test_loss_epoch)}\")\n",
    "    \n",
    "print(f\"Last LR: {scheduler.get_last_lr()}\")\n",
    "print(f\"Best Accuracy: Train {np.max(train_accuracy)}, Val {np.max(test_accuracy)}\")\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(train_loss, label=\"Train loss\")\n",
    "plt.plot(test_loss, label=\"Validation loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(train_accuracy, label=\"Train accuracy\")\n",
    "plt.plot(test_accuracy, label=\"Validation accuracy\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = torch.nn.Softmax(dim=1)\n",
    "output = m(out_zones)\n",
    "accuracy = accuracy_score(data[\"zones\"].y.cpu().reshape(-1).type(torch.long), np.array(torch.argmax(output.cpu(), axis=1)))\n",
    "\n",
    "print(accuracy)\n",
    "print(classification_report(data[\"zones\"].y.cpu().reshape(-1).type(torch.long), np.array(torch.argmax(output.cpu(), axis=1))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Análisis sacando una zona"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La idea de este análisis es ver qué tan robusto es el algoritmo cuando se agrega una nueva zona. Lo interesante es ver si al agregar la nueva zona, sin reentrenar la red se tiene un buen desempeño."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "batch_size = 16\n",
    "learning_rate = 0.01 \n",
    "print_every = 50\n",
    "\n",
    "zones_to_remove = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
    "accuracy = {\"0\":[],\"1\":[],\"2\":[],\"3\":[],\"4\":[],\"5\":[],\"6\":[],\"7\":[],\"8\":[],\"9\":[],\"10\":[],\"11\":[],\"12\":[],\"13\":[],\"14\":[],\"15\":[],}\n",
    "\n",
    "for zone_to_remove in zones_to_remove:\n",
    "    print('Zone removed: ', zone_to_remove)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test, num_aps, num_classes, enc_train = preprocess_dataset(dataset, zone_to_remove=zone_to_remove)\n",
    "    ap_edge_index, ap_edge_attr = ap_graph_creator(X_train[:,:15], th=10) #el grafo lo armo solo con los datos de 2.4Ghz\n",
    "    zone_ap_edge_index, zone_ap_edge_attr = zone_ap_graph_creator(X_train[:,:15], y_train, zone_to_remove=zone_to_remove) #el grafo lo armo solo con los datos de 2.4Ghz\n",
    "    data = build_heterodata(ap_edge_index, ap_edge_attr, zone_ap_edge_index, zone_ap_edge_attr, num_classes)\n",
    "    T.ToUndirected()(data)\n",
    "\n",
    "    # Armado del dataset\n",
    "\n",
    "    x_training_data = np.reshape(X_train,(X_train.shape[0],15,2))\n",
    "    x_test_data = np.reshape(X_test,(X_test.shape[0],15,2))\n",
    "    y_training_data = y_train\n",
    "    y_test_data = y_test\n",
    "\n",
    "    #normalize (x-mean)/std\n",
    "    mean = x_training_data.mean(axis=0)\n",
    "    std = x_training_data.std(axis=0)\n",
    "\n",
    "    x_training_data = x_training_data - mean\n",
    "    x_training_data /= std\n",
    "    x_test_data = x_test_data - mean\n",
    "    x_test_data /= std \n",
    "\n",
    "    train_dataset = build_dataset(x_training_data, y_training_data, data)\n",
    "    test_dataset = build_dataset(x_test_data, y_test_data, data) \n",
    "        \n",
    "    for _ in range(1):\n",
    "\n",
    "        model = HeteroGNN_simplified(20, 1, 4).to(device)\n",
    "\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=len(test_dataset), shuffle=False)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=5e-4)\n",
    "        loss = torch.nn.CrossEntropyLoss()\n",
    "        scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.8)\n",
    "\n",
    "        train_loss = []\n",
    "        train_accuracy = []\n",
    "        test_loss = []\n",
    "        test_accuracy = []\n",
    "        best_test_accuracy = 0\n",
    "\n",
    "\n",
    "        m = torch.nn.Softmax(dim=1)\n",
    "\n",
    "        for epoch in range(150):\n",
    "            # print(f\"Epoch: {epoch+1}\")\n",
    "\n",
    "            # TRAIN\n",
    "            model.train()\n",
    "            train_accuracy_epoch = []\n",
    "            train_loss_epoch = []\n",
    "            for data in train_loader:\n",
    "\n",
    "                data = data.to(device)\n",
    "                out = model(data.x_dict, data.edge_index_dict, data.edge_attr_dict) \n",
    "                out_zones = out[\"zones\"].cpu().reshape(out[\"zones\"].cpu().shape[0]//num_classes,num_classes)\n",
    "\n",
    "                loss_result = loss(out_zones.cpu(), data[\"zones\"].y.cpu().type(torch.long))\n",
    "                loss_result.backward()\n",
    "                train_loss_epoch.append(loss_result.detach().cpu())\n",
    "\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                output = m(out_zones)\n",
    "                train_accuracy_epoch.append(accuracy_score(data[\"zones\"].y.cpu().reshape(-1).type(torch.long), np.array(torch.argmax(output.cpu(), axis=1))))\n",
    "\n",
    "            # if scheduler.get_last_lr()[0] > 0.0005:\n",
    "            if (epoch+1)%20 == 0:\n",
    "                scheduler.step()\n",
    "\n",
    "            train_accuracy.append(np.mean(train_accuracy_epoch))\n",
    "            train_loss.append(np.mean(train_loss_epoch))\n",
    "\n",
    "\n",
    "\n",
    "            # VALIDATION\n",
    "            model.eval()\n",
    "            test_accuracy_epoch = []\n",
    "            test_loss_epoch = []\n",
    "            for data in test_loader:\n",
    "\n",
    "                data = data.to(device)\n",
    "                out = model(data.x_dict, data.edge_index_dict, data.edge_attr_dict) \n",
    "                out_zones = out[\"zones\"].cpu().reshape(out[\"zones\"].cpu().shape[0]//num_classes,num_classes)\n",
    "                loss_result = loss(out_zones.cpu(), data[\"zones\"].y.cpu().type(torch.long))        \n",
    "                test_loss_epoch.append(loss_result.detach().cpu())\n",
    "\n",
    "                output = m(out_zones)\n",
    "                test_accuracy_epoch.append(accuracy_score(data[\"zones\"].y.cpu().reshape(-1).type(torch.long), np.array(torch.argmax(output.cpu(), axis=1))))\n",
    "\n",
    "            test_accuracy.append(np.mean(test_accuracy_epoch))\n",
    "            if test_accuracy[-1] > best_test_accuracy:\n",
    "                best_test_accuracy = test_accuracy[-1]        \n",
    "                torch.save(model.state_dict(), f\"MNAV_HeteroGNN_simplified_without_{zone_to_remove}_best_model_2.pth\")\n",
    "\n",
    "            test_loss.append(np.mean(test_loss_epoch))\n",
    "\n",
    "            # if (epoch+1)%print_every == 0:\n",
    "            #     print(f\"Epoch {epoch+1}, Train Loss {np.mean(train_loss_epoch)}, Val Loss {np.mean(test_loss_epoch)}\")\n",
    "\n",
    "        print(f\"Last LR: {scheduler.get_last_lr()}\")\n",
    "        print(f\"Best Accuracy: Train {np.max(train_accuracy)}, Val {np.max(test_accuracy)}\") \n",
    "    \n",
    "        accuracy[str(zone_to_remove)].append(np.max(test_accuracy))\n",
    "\n",
    "print(accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Segunda prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0')\n",
    "batch_size = 16\n",
    "learning_rate = 0.01 \n",
    "print_every = 50\n",
    "\n",
    "zones_to_remove = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
    "accuracy = {\"0\":[],\"1\":[],\"2\":[],\"3\":[],\"4\":[],\"5\":[],\"6\":[],\"7\":[],\"8\":[],\"9\":[],\"10\":[],\"11\":[],\"12\":[],\"13\":[],\"14\":[],\"15\":[],}\n",
    "\n",
    "for zone_to_remove in zones_to_remove:\n",
    "    print('Zone removed: ', zone_to_remove)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test, num_aps, num_classes, enc_train = preprocess_dataset(dataset)\n",
    "    ap_edge_index, ap_edge_attr = ap_graph_creator(X_train[:,:15], th=10) #el grafo lo armo solo con los datos de 2.4Ghz\n",
    "    zone_ap_edge_index, zone_ap_edge_attr = zone_ap_graph_creator(X_train[:,:15], y_train, zone_to_remove=zone_to_remove) #el grafo lo armo solo con los datos de 2.4Ghz\n",
    "    data = build_heterodata(ap_edge_index, ap_edge_attr, zone_ap_edge_index, zone_ap_edge_attr, num_classes)\n",
    "    T.ToUndirected()(data)\n",
    "\n",
    "    # Armado del dataset\n",
    "\n",
    "    x_training_data = np.reshape(X_train,(X_train.shape[0],15,2))\n",
    "    x_test_data = np.reshape(X_test,(X_test.shape[0],15,2))\n",
    "    y_training_data = y_train\n",
    "    y_test_data = y_test\n",
    "\n",
    "    #normalize (x-mean)/std\n",
    "    mean = x_training_data.mean(axis=0)\n",
    "    std = x_training_data.std(axis=0)\n",
    "\n",
    "    x_training_data = x_training_data - mean\n",
    "    x_training_data /= std\n",
    "    x_test_data = x_test_data - mean\n",
    "    x_test_data /= std \n",
    "\n",
    "    train_dataset = build_dataset(x_training_data, y_training_data, data, zone_to_remove=zone_to_remove)\n",
    "    test_dataset = build_dataset(x_test_data, y_test_data, data, zone_to_remove=zone_to_remove) \n",
    "        \n",
    "    for _ in range(1):\n",
    "\n",
    "        model = HeteroGNN_simplified(20, 1, 4).to(device)\n",
    "\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=len(test_dataset), shuffle=False)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=5e-4)\n",
    "        loss = torch.nn.CrossEntropyLoss()\n",
    "        scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.8)\n",
    "\n",
    "        train_loss = []\n",
    "        train_accuracy = []\n",
    "        test_loss = []\n",
    "        test_accuracy = []\n",
    "        best_test_accuracy = 0\n",
    "\n",
    "\n",
    "        m = torch.nn.Softmax(dim=1)\n",
    "\n",
    "        for epoch in range(150):\n",
    "            # print(f\"Epoch: {epoch+1}\")\n",
    "\n",
    "            # TRAIN\n",
    "            model.train()\n",
    "            train_accuracy_epoch = []\n",
    "            train_loss_epoch = []\n",
    "            for data in train_loader:\n",
    "\n",
    "                data = data.to(device)\n",
    "                out = model(data.x_dict, data.edge_index_dict, data.edge_attr_dict) \n",
    "                out_zones = out[\"zones\"].cpu().reshape(out[\"zones\"].cpu().shape[0]//num_classes,num_classes)\n",
    "\n",
    "                loss_result = loss(out_zones.cpu(), data[\"zones\"].y.cpu().type(torch.long))\n",
    "                loss_result.backward()\n",
    "                train_loss_epoch.append(loss_result.detach().cpu())\n",
    "\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                output = m(out_zones)\n",
    "                train_accuracy_epoch.append(accuracy_score(data[\"zones\"].y.cpu().reshape(-1).type(torch.long), np.array(torch.argmax(output.cpu(), axis=1))))\n",
    "\n",
    "            # if scheduler.get_last_lr()[0] > 0.0005:\n",
    "            if (epoch+1)%20 == 0:\n",
    "                scheduler.step()\n",
    "\n",
    "            train_accuracy.append(np.mean(train_accuracy_epoch))\n",
    "            train_loss.append(np.mean(train_loss_epoch))\n",
    "\n",
    "\n",
    "\n",
    "            # VALIDATION\n",
    "            model.eval()\n",
    "            test_accuracy_epoch = []\n",
    "            test_loss_epoch = []\n",
    "            for data in test_loader:\n",
    "\n",
    "                data = data.to(device)\n",
    "                out = model(data.x_dict, data.edge_index_dict, data.edge_attr_dict) \n",
    "                out_zones = out[\"zones\"].cpu().reshape(out[\"zones\"].cpu().shape[0]//num_classes,num_classes)\n",
    "                loss_result = loss(out_zones.cpu(), data[\"zones\"].y.cpu().type(torch.long))        \n",
    "                test_loss_epoch.append(loss_result.detach().cpu())\n",
    "\n",
    "                output = m(out_zones)\n",
    "                test_accuracy_epoch.append(accuracy_score(data[\"zones\"].y.cpu().reshape(-1).type(torch.long), np.array(torch.argmax(output.cpu(), axis=1))))\n",
    "\n",
    "            test_accuracy.append(np.mean(test_accuracy_epoch))\n",
    "            if test_accuracy[-1] > best_test_accuracy:\n",
    "                best_test_accuracy = test_accuracy[-1]        \n",
    "                torch.save(model.state_dict(), f\"MNAV_HeteroGNN_simplified_without_{zone_to_remove}_best_model_2.pth\")\n",
    "\n",
    "            test_loss.append(np.mean(test_loss_epoch))\n",
    "\n",
    "            # if (epoch+1)%print_every == 0:\n",
    "            #     print(f\"Epoch {epoch+1}, Train Loss {np.mean(train_loss_epoch)}, Val Loss {np.mean(test_loss_epoch)}\")\n",
    "\n",
    "        print(f\"Last LR: {scheduler.get_last_lr()}\")\n",
    "        print(f\"Best Accuracy: Train {np.max(train_accuracy)}, Val {np.max(test_accuracy)}\") \n",
    "    \n",
    "        accuracy[str(zone_to_remove)].append(np.max(test_accuracy))\n",
    "\n",
    "print(accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split de los datos y armado del objeto Data con el grafo\n",
    "\n",
    "X_train, X_test, y_train, y_test, num_aps, num_classes, enc_train = preprocess_dataset(dataset)\n",
    "ap_edge_index, ap_edge_attr = ap_graph_creator(X_train[:,:15], th=10) #el grafo lo armo solo con los datos de 2.4Ghz\n",
    "zone_ap_edge_index, zone_ap_edge_attr = zone_ap_graph_creator(X_train[:,:15], y_train) #el grafo lo armo solo con los datos de 2.4Ghz\n",
    "data = build_heterodata(ap_edge_index, ap_edge_attr, zone_ap_edge_index, zone_ap_edge_attr)\n",
    "T.ToUndirected()(data)\n",
    "\n",
    "# Armado del dataset\n",
    "\n",
    "x_training_data = np.reshape(X_train,(X_train.shape[0],15,2))\n",
    "x_test_data = np.reshape(X_test,(X_test.shape[0],15,2))\n",
    "y_training_data = y_train\n",
    "y_test_data = y_test\n",
    "\n",
    "#normalize (x-mean)/std\n",
    "mean = x_training_data.mean(axis=0)\n",
    "std = x_training_data.std(axis=0)\n",
    "\n",
    "x_training_data = x_training_data - mean\n",
    "x_training_data /= std\n",
    "x_test_data = x_test_data - mean\n",
    "x_test_data /= std\n",
    "\n",
    "train_dataset = build_dataset(x_training_data, y_training_data, data)\n",
    "test_dataset = build_dataset(x_test_data, y_test_data, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zones_to_remove = [6] #, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
    "for zone_to_remove in zones_to_remove:\n",
    "    print('Zone removed: ', zone_to_remove)\n",
    "    # model = HeteroGNN(20, 1, 4)\n",
    "    model = HeteroGNN_simplified(20, 1, 4)\n",
    "    model.load_state_dict(torch.load(f\"MNAV_HeteroGNN_simplified_without_{zone_to_remove}_best_model.pth\"))\n",
    "    model.to(device)\n",
    "\n",
    "    test_loader = DataLoader(test_dataset, batch_size=len(test_dataset), shuffle=False)\n",
    "    \n",
    "    # VALIDATION\n",
    "    model.eval()\n",
    "\n",
    "    for data in test_loader:\n",
    "        data = data.to(device)\n",
    "        data.x_dict[\"aps\"].requires_grad_()\n",
    "        data.x_dict[\"zones\"].requires_grad_()        \n",
    "        out = model(data.x_dict, data.edge_index_dict, data.edge_attr_dict) \n",
    "        out_zones = out[\"zones\"].cpu().reshape(out[\"zones\"].cpu().shape[0]//num_classes,num_classes)\n",
    "\n",
    "        scores = (out_zones.gather(1, data[\"zones\"].y.cpu().view(-1, 1).type(torch.long)).squeeze())\n",
    "        scores.backward(torch.FloatTensor([1.0]*scores.shape[0]))\n",
    "        saliency, _ = torch.max(data.x_dict[\"aps\"].grad.data.abs(), dim=1)\n",
    "\n",
    "        \n",
    "        output = m(out_zones)\n",
    "        print(classification_report(data[\"zones\"].y.cpu().reshape(-1).type(torch.long), np.array(torch.argmax(output.cpu(), axis=1))))\n",
    "        \n",
    "        for zone in range(num_classes):\n",
    "            # mask_gt = data[\"zones\"].y.cpu() == zone\n",
    "            # saliency_removed_zone = saliency.reshape((len(test_dataset),num_aps))[mask_gt]\n",
    "            # sal = np.mean(saliency_removed_zone.cpu().numpy(), axis=0).reshape(num_aps,1)\n",
    "\n",
    "            mask_pred = np.array(torch.argmax(output.cpu(), axis=1)) == zone\n",
    "            saliency_removed_zone_pred = saliency.reshape((len(test_dataset),num_aps))[mask_pred]\n",
    "            sal = np.mean(saliency_removed_zone_pred.cpu().numpy(), axis=0).reshape(num_aps,1)\n",
    "            if zone == 0:\n",
    "                sal_stack = sal\n",
    "            else:\n",
    "                sal_stack = np.concatenate((sal_stack, sal))\n",
    "\n",
    "        \n",
    "        # import ipdb; ipdb.set_trace()\n",
    "        \n",
    "        # plt.figure(figsize=[9,7])\n",
    "        # cf_matrix = confusion_matrix(data[\"zones\"].y.cpu().reshape(-1).type(torch.long), np.array(torch.argmax(output.cpu(), axis=1)), normalize=\"true\")\n",
    "        # sns.heatmap(cf_matrix, annot=True, fmt=\".0%\", cmap=\"YlGnBu\", vmin=0, vmax=0.2, cbar=False)\n",
    "        # plt.figure(figsize=[9,7])\n",
    "        # sns.heatmap(saliency_removed_zone.cpu().numpy(), annot=False, fmt=\".0%\", cmap=\"YlGnBu\", cbar=True)\n",
    "        plt.figure(figsize=[9,7])\n",
    "        sns.heatmap(sal_stack.reshape((15,num_classes)), annot=False, fmt=\".0%\", cmap=\"YlGnBu\", cbar=True)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Grafo con una zona menos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zone_to_remove = 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split de los datos y armado del objeto Data con el grafo\n",
    "\n",
    "X_train, X_test, y_train, y_test, num_aps, num_classes, enc_train = preprocess_dataset(dataset, zone_to_remove=zone_to_remove)\n",
    "ap_edge_index, ap_edge_attr = ap_graph_creator(X_train[:,:15], th=10) #el grafo lo armo solo con los datos de 2.4Ghz\n",
    "zone_ap_edge_index, zone_ap_edge_attr = zone_ap_graph_creator(X_train[:,:15], y_train, zone_to_remove=zone_to_remove) #el grafo lo armo solo con los datos de 2.4Ghz\n",
    "data = build_heterodata(ap_edge_index, ap_edge_attr, zone_ap_edge_index, zone_ap_edge_attr, num_classes)\n",
    "T.ToUndirected()(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Armado del dataset\n",
    "\n",
    "x_training_data = np.reshape(X_train,(X_train.shape[0],15,2))\n",
    "x_test_data = np.reshape(X_test,(X_test.shape[0],15,2))\n",
    "y_training_data = y_train\n",
    "y_test_data = y_test\n",
    "\n",
    "#normalize (x-mean)/std\n",
    "mean = x_training_data.mean(axis=0)\n",
    "std = x_training_data.std(axis=0)\n",
    "\n",
    "x_training_data = x_training_data - mean\n",
    "x_training_data /= std\n",
    "x_test_data = x_test_data - mean\n",
    "x_test_data /= std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = build_dataset(x_training_data, y_training_data, data, zone_to_remove)\n",
    "test_dataset = build_dataset(x_test_data, y_test_data, data, zone_to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = HeteroGNN(20, 1, 4).to(device)\n",
    "model = HeteroGNN_simplified(20, 1, 4).to(device)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=len(test_dataset), shuffle=False)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=5e-4)\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.8)\n",
    "print_every = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = []\n",
    "train_accuracy = []\n",
    "test_loss = []\n",
    "test_accuracy = []\n",
    "best_test_accuracy = 0\n",
    "\n",
    "\n",
    "m = torch.nn.Softmax(dim=1)\n",
    "\n",
    "for epoch in range(150):\n",
    "    # print(f\"Epoch: {epoch+1}\")\n",
    "    \n",
    "    # TRAIN\n",
    "    model.train()\n",
    "    train_accuracy_epoch = []\n",
    "    train_loss_epoch = []\n",
    "    for data in train_loader:\n",
    "        # import ipdb; ipdb.set_trace()\n",
    "        \n",
    "        data = data.to(device)\n",
    "        \n",
    "        out = model(data.x_dict, data.edge_index_dict, data.edge_attr_dict) \n",
    "        \n",
    "        out_zones = out[\"zones\"].cpu().reshape(out[\"zones\"].cpu().shape[0]//num_classes,num_classes)\n",
    "        loss_result = loss(out_zones.cpu(), data[\"zones\"].y.cpu().type(torch.long))\n",
    "        loss_result.backward()\n",
    "        train_loss_epoch.append(loss_result.detach().cpu())\n",
    "        \n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = m(out_zones)\n",
    "        train_accuracy_epoch.append(accuracy_score(data[\"zones\"].y.cpu().reshape(-1).type(torch.long), np.array(torch.argmax(output.cpu(), axis=1))))\n",
    "\n",
    "    # if scheduler.get_last_lr()[0] > 0.0005:\n",
    "    if (epoch+1)%10 == 0:\n",
    "        scheduler.step()\n",
    "\n",
    "    train_accuracy.append(np.mean(train_accuracy_epoch))\n",
    "    train_loss.append(np.mean(train_loss_epoch))\n",
    "    \n",
    "    \n",
    "    \n",
    "    # VALIDATION\n",
    "    model.eval()\n",
    "    test_accuracy_epoch = []\n",
    "    test_loss_epoch = []\n",
    "    for data in test_loader:\n",
    "        \n",
    "        data = data.to(device)\n",
    "        out = model(data.x_dict, data.edge_index_dict, data.edge_attr_dict) \n",
    "        # out_zones = torch.mean(out[\"zones\"].cpu(),1)\n",
    "        # out_zones = out_zones.reshape(out_zones.shape[0]//num_classes,num_classes)\n",
    "        out_zones = out[\"zones\"].cpu().reshape(out[\"zones\"].cpu().shape[0]//num_classes,num_classes)\n",
    "\n",
    "        # out_zones = out.cpu().reshape(out.cpu().shape[0]//num_classes,num_classes)\n",
    "        \n",
    "        loss_result = loss(out_zones.cpu(), data[\"zones\"].y.cpu().type(torch.long))        \n",
    "        test_loss_epoch.append(loss_result.detach().cpu())\n",
    "        \n",
    "        output = m(out_zones)\n",
    "        test_accuracy_epoch.append(accuracy_score(data[\"zones\"].y.cpu().reshape(-1).type(torch.long), np.array(torch.argmax(output.cpu(), axis=1))))\n",
    "\n",
    "    test_accuracy.append(np.mean(test_accuracy_epoch))\n",
    "    if test_accuracy[-1] > best_test_accuracy:\n",
    "        best_test_accuracy = test_accuracy[-1]        \n",
    "        torch.save(model.state_dict(), \"MNAV_HeteroGNN_removed_zone_best_model.pth\")\n",
    "        \n",
    "    test_loss.append(np.mean(test_loss_epoch))\n",
    "\n",
    "    if (epoch+1)%print_every == 0:\n",
    "        print(f\"Epoch {epoch+1}, Train Loss {np.mean(train_loss_epoch)}, Val Loss {np.mean(test_loss_epoch)}\")\n",
    "    \n",
    "print(f\"Last LR: {scheduler.get_last_lr()}\")\n",
    "print(f\"Best Accuracy: Train {np.max(train_accuracy)}, Val {np.max(test_accuracy)}\")\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(train_loss, label=\"Train loss\")\n",
    "plt.plot(test_loss, label=\"Validation loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(train_accuracy, label=\"Train accuracy\")\n",
    "plt.plot(test_accuracy, label=\"Validation accuracy\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(classification_report(data[\"zones\"].y.cpu().reshape(-1).type(torch.long), np.array(torch.argmax(output.cpu(), axis=1))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Test con todas las zonas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split de los datos y armado del objeto Data con el grafo\n",
    "\n",
    "X_train, X_test, y_train, y_test, num_aps, num_classes, enc_train = preprocess_dataset(dataset)\n",
    "ap_edge_index, ap_edge_attr = ap_graph_creator(X_train[:,:15], th=10) #el grafo lo armo solo con los datos de 2.4Ghz\n",
    "zone_ap_edge_index, zone_ap_edge_attr = zone_ap_graph_creator(X_train[:,:15], y_train) #el grafo lo armo solo con los datos de 2.4Ghz\n",
    "data = build_heterodata(ap_edge_index, ap_edge_attr, zone_ap_edge_index, zone_ap_edge_attr)\n",
    "T.ToUndirected()(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Armado del dataset\n",
    "\n",
    "x_training_data = np.reshape(X_train,(X_train.shape[0],15,2))\n",
    "x_test_data = np.reshape(X_test,(X_test.shape[0],15,2))\n",
    "y_training_data = y_train\n",
    "y_test_data = y_test\n",
    "\n",
    "#normalize (x-mean)/std\n",
    "mean = x_training_data.mean(axis=0)\n",
    "std = x_training_data.std(axis=0)\n",
    "\n",
    "x_training_data = x_training_data - mean\n",
    "x_training_data /= std\n",
    "x_test_data = x_test_data - mean\n",
    "x_test_data /= std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = build_dataset(x_training_data, y_training_data, data)\n",
    "test_dataset = build_dataset(x_test_data, y_test_data, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for zone_to_remove in zones_to_remove:\n",
    "    print('Zone removed: ', zone_to_remove)\n",
    "    # model = HeteroGNN(20, 1, 4)\n",
    "    model = HeteroGNN_simplified(20, 1, 4)\n",
    "    model.load_state_dict(torch.load(f\"MNAV_HeteroGNN_simplified_without_{zone_to_remove}_best_model.pth\"))\n",
    "    model.to(device)\n",
    "\n",
    "    test_loader = DataLoader(test_dataset, batch_size=len(test_dataset), shuffle=False)\n",
    "    \n",
    "    # VALIDATION\n",
    "    model.eval()\n",
    "\n",
    "    for data in test_loader:\n",
    "        data = data.to(device)\n",
    "        out = model(data.x_dict, data.edge_index_dict, data.edge_attr_dict) \n",
    "        out_zones = out[\"zones\"].cpu().reshape(out[\"zones\"].cpu().shape[0]//num_classes,num_classes)\n",
    "\n",
    "        output = m(out_zones)\n",
    "        print(classification_report(data[\"zones\"].y.cpu().reshape(-1).type(torch.long), np.array(torch.argmax(output.cpu(), axis=1))))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "zona 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VALIDATION\n",
    "model.eval()\n",
    "\n",
    "for data in test_loader:\n",
    "    data = data.to(device)\n",
    "    out = model(data.x_dict, data.edge_index_dict, data.edge_attr_dict) \n",
    "    out_zones = out[\"zones\"].cpu().reshape(out[\"zones\"].cpu().shape[0]//num_classes,num_classes)\n",
    "\n",
    "    output = m(out_zones)\n",
    "    print(classification_report(data[\"zones\"].y.cpu().reshape(-1).type(torch.long), np.array(torch.argmax(output.cpu(), axis=1))))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[9,7])\n",
    "cf_matrix = confusion_matrix(data[\"zones\"].y.cpu().reshape(-1).type(torch.long), np.array(torch.argmax(output.cpu(), axis=1)), normalize=\"true\")\n",
    "sns.heatmap(cf_matrix, annot=True, fmt=\".0%\", cmap=\"YlGnBu\", vmin=0, vmax=0.2, cbar=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "zona 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VALIDATION\n",
    "model.eval()\n",
    "\n",
    "for data in test_loader:\n",
    "    data = data.to(device)\n",
    "    out = model(data.x_dict, data.edge_index_dict, data.edge_attr_dict) \n",
    "    out_zones = out[\"zones\"].cpu().reshape(out[\"zones\"].cpu().shape[0]//num_classes,num_classes)\n",
    "\n",
    "    output = m(out_zones)\n",
    "    print(classification_report(data[\"zones\"].y.cpu().reshape(-1).type(torch.long), np.array(torch.argmax(output.cpu(), axis=1))))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[9,7])\n",
    "cf_matrix = confusion_matrix(data[\"zones\"].y.cpu().reshape(-1).type(torch.long), np.array(torch.argmax(output.cpu(), axis=1)), normalize=\"true\")\n",
    "sns.heatmap(cf_matrix, annot=True, fmt=\".0%\", cmap=\"YlGnBu\", vmin=0, vmax=0.2, cbar=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for zone_to_remove in zones_to_remove:\n",
    "    print('Zone removed: ', zone_to_remove)\n",
    "    # model = HeteroGNN(20, 1, 4)\n",
    "    model = HeteroGNN_simplified(20, 1, 4)\n",
    "    model.load_state_dict(torch.load(f\"MNAV_HeteroGNN_simplified_without_{zone_to_remove}_best_model.pth\"))\n",
    "    model.to(device)\n",
    "\n",
    "    test_loader = DataLoader(test_dataset, batch_size=len(test_dataset), shuffle=False)\n",
    "    \n",
    "    # VALIDATION\n",
    "    model.eval()\n",
    "\n",
    "    for data in test_loader:\n",
    "        data = data.to(device)\n",
    "        out = model(data.x_dict, data.edge_index_dict, data.edge_attr_dict) \n",
    "        out_zones = out[\"zones\"].cpu().reshape(out[\"zones\"].cpu().shape[0]//num_classes,num_classes)\n",
    "\n",
    "        output = m(out_zones)\n",
    "        print(classification_report(data[\"zones\"].y.cpu().reshape(-1).type(torch.long), np.array(torch.argmax(output.cpu(), axis=1))))\n",
    "        plt.figure(figsize=[9,7])\n",
    "        cf_matrix = confusion_matrix(data[\"zones\"].y.cpu().reshape(-1).type(torch.long), np.array(torch.argmax(output.cpu(), axis=1)), normalize=\"true\")\n",
    "        sns.heatmap(cf_matrix, annot=True, fmt=\".0%\", cmap=\"YlGnBu\", vmin=0, vmax=0.2, cbar=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Análisis variando cantidad de muestras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### HeteroGNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "batch_size = 16\n",
    "learning_rate = 0.01  \n",
    "print_every = 5\n",
    "\n",
    "porcentajes = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
    "accuracy = {\"0.1\":[], \"0.2\":[], \"0.3\":[], \"0.4\":[], \"0.5\":[], \"0.6\":[], \"0.7\":[], \"0.8\":[], \"0.9\":[], \"1\":[]}\n",
    "\n",
    "for porc in porcentajes:\n",
    "    print('Porcentaje de datos: ', porc)\n",
    "    \n",
    "    for i in range(10):    \n",
    "\n",
    "        # Split de los datos y armado del objeto Data con el grafo\n",
    "        X_train, X_test, y_train, y_test, num_aps, num_classes, enc_train = preprocess_dataset(dataset, dataset_percentage=porc)\n",
    "        ap_edge_index, ap_edge_attr = ap_graph_creator(X_train[:,:15], th=10, prune_th=10) #el grafo lo armo solo con los datos de 2.4Ghz\n",
    "        zone_ap_edge_index, zone_ap_edge_attr = zone_ap_graph_creator(X_train[:,:15], y_train, prune_th=10) #el grafo lo armo solo con los datos de 2.4Ghz\n",
    "        data = build_heterodata(ap_edge_index, ap_edge_attr, zone_ap_edge_index, zone_ap_edge_attr, num_classes)\n",
    "        T.ToUndirected()(data)\n",
    "\n",
    "        # Armado del dataset\n",
    "\n",
    "        x_training_data = np.reshape(X_train,(X_train.shape[0],15,2))\n",
    "        x_test_data = np.reshape(X_test,(X_test.shape[0],15,2))\n",
    "        y_training_data = y_train\n",
    "        y_test_data = y_test\n",
    "\n",
    "        #normalize (x-mean)/std\n",
    "        mean = x_training_data.mean(axis=0)\n",
    "        std = x_training_data.std(axis=0)\n",
    "\n",
    "        x_training_data = x_training_data - mean\n",
    "        x_training_data /= std\n",
    "        x_test_data = x_test_data - mean\n",
    "        x_test_data /= std   \n",
    "\n",
    "        train_dataset = build_dataset(x_training_data, y_training_data, data)\n",
    "        test_dataset = build_dataset(x_test_data, y_test_data, data) \n",
    "\n",
    "        model = HeteroGNN(20, 1, 4).to(device)\n",
    "\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=len(test_dataset), shuffle=False)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=5e-4)\n",
    "        loss = torch.nn.CrossEntropyLoss()\n",
    "        scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.8)\n",
    "\n",
    "        train_loss = []\n",
    "        train_accuracy = []\n",
    "        test_loss = []\n",
    "        test_accuracy = []\n",
    "        best_test_accuracy = 0\n",
    "\n",
    "        m = torch.nn.Softmax(dim=1)\n",
    "\n",
    "        for epoch in range(150):\n",
    "            # print(f\"Epoch: {epoch+1}\")\n",
    "\n",
    "            # TRAIN\n",
    "            model.train()\n",
    "            train_accuracy_epoch = []\n",
    "            train_loss_epoch = []\n",
    "            for data in train_loader:\n",
    "                # import ipdb; ipdb.set_trace()\n",
    "\n",
    "                data = data.to(device)\n",
    "\n",
    "                out = model(data.x_dict, data.edge_index_dict, data.edge_attr_dict) \n",
    "\n",
    "                out_zones = out[\"zones\"].cpu().reshape(out[\"zones\"].cpu().shape[0]//num_classes,num_classes)\n",
    "                loss_result = loss(out_zones.cpu(), data[\"zones\"].y.cpu().type(torch.long))\n",
    "                loss_result.backward()\n",
    "                train_loss_epoch.append(loss_result.detach().cpu())\n",
    "\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                output = m(out_zones)\n",
    "                train_accuracy_epoch.append(accuracy_score(data[\"zones\"].y.cpu().reshape(-1).type(torch.long), np.array(torch.argmax(output.cpu(), axis=1))))\n",
    "\n",
    "            # if scheduler.get_last_lr()[0] > 0.0005:\n",
    "            if (epoch+1)%10 == 0:\n",
    "                scheduler.step()\n",
    "\n",
    "            train_accuracy.append(np.mean(train_accuracy_epoch))\n",
    "            train_loss.append(np.mean(train_loss_epoch))\n",
    "\n",
    "            # VALIDATION\n",
    "            model.eval()\n",
    "            test_accuracy_epoch = []\n",
    "            test_loss_epoch = []\n",
    "            for data in test_loader:\n",
    "\n",
    "                data = data.to(device)\n",
    "                out = model(data.x_dict, data.edge_index_dict, data.edge_attr_dict) \n",
    "                # out_zones = torch.mean(out[\"zones\"].cpu(),1)\n",
    "                # out_zones = out_zones.reshape(out_zones.shape[0]//num_classes,num_classes)\n",
    "                out_zones = out[\"zones\"].cpu().reshape(out[\"zones\"].cpu().shape[0]//num_classes,num_classes)\n",
    "\n",
    "                # out_zones = out.cpu().reshape(out.cpu().shape[0]//num_classes,num_classes)\n",
    "\n",
    "                loss_result = loss(out_zones.cpu(), data[\"zones\"].y.cpu().type(torch.long))        \n",
    "                test_loss_epoch.append(loss_result.detach().cpu())\n",
    "\n",
    "                output = m(out_zones)\n",
    "                test_accuracy_epoch.append(accuracy_score(data[\"zones\"].y.cpu().reshape(-1).type(torch.long), np.array(torch.argmax(output.cpu(), axis=1))))\n",
    "\n",
    "            test_accuracy.append(np.mean(test_accuracy_epoch))\n",
    "            test_loss.append(np.mean(test_loss_epoch))\n",
    "\n",
    "        print(f\"Best Accuracy: Train {np.max(train_accuracy)}, Val {np.max(test_accuracy)}\")\n",
    "        accuracy[str(porc)].append(np.max(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = {'0.3': [0.9364069952305246, 0.9268680445151033, 0.918918918918919, 0.9364069952305246, 0.9364069952305246], '0.4': [0.9534606205250596, 0.9486873508353222, 0.9486873508353222, 0.951073985680191, 0.9427207637231504], '0.5': [0.9474689589302769, 0.9398280802292264, 0.944603629417383, 0.944603629417383, 0.9474689589302769], '0.6': [0.954653937947494, 0.964200477326969, 0.964200477326969, 0.9562450278440732, 0.9578361177406524], '0.7': [0.9577080491132333, 0.9604365620736699, 0.9624829467939973, 0.961118690313779, 0.9556616643929059], '0.8': [0.9498507462686567, 0.9552238805970149, 0.9582089552238806, 0.955820895522388, 0.955820895522388], '0.9': [0.9639257294429708, 0.9612732095490716, 0.9671087533156498, 0.96657824933687, 0.96657824933687], '1': [0.9737344794651385, 0.9713467048710601, 0.9737344794651385, 0.9746895893027698, 0.9680038204393505]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux = []\n",
    "aux.append(accuracy['0.3'])\n",
    "aux.append(accuracy['0.4'])\n",
    "aux.append(accuracy['0.5'])\n",
    "aux.append(accuracy['0.6'])\n",
    "aux.append(accuracy['0.7'])\n",
    "aux.append(accuracy['0.8'])\n",
    "aux.append(accuracy['0.9'])\n",
    "aux.append(accuracy['1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[9,7])\n",
    "plt.boxplot(aux, showfliers=False) #, meanline=True, showmeans=True)\n",
    "plt.xticks([1, 2, 3, 4, 5, 6, 7, 8], [30, 40, 50, 60, 70, 80, 90, 100])\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Fingerprints sample size (%)')\n",
    "plt.grid()\n",
    "plt.savefig('MNAV_hetero_cant_muestras.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### HeteroGNNSimplified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "batch_size = 16\n",
    "learning_rate = 0.01  \n",
    "print_every = 5\n",
    "\n",
    "porcentajes = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
    "accuracy = {\"0.1\":[], \"0.2\":[], \"0.3\":[], \"0.4\":[], \"0.5\":[], \"0.6\":[], \"0.7\":[], \"0.8\":[], \"0.9\":[], \"1\":[]}\n",
    "\n",
    "for porc in porcentajes:\n",
    "    print('Porcentaje de datos: ', porc)   \n",
    "    \n",
    "    for i in range(10):    \n",
    "        \n",
    "        # Split de los datos y armado del objeto Data con el grafo\n",
    "        X_train, X_test, y_train, y_test, num_aps, num_classes, enc_train = preprocess_dataset(dataset, dataset_percentage=porc)\n",
    "        ap_edge_index, ap_edge_attr = ap_graph_creator(X_train[:,:15], th=10) #el grafo lo armo solo con los datos de 2.4Ghz\n",
    "        zone_ap_edge_index, zone_ap_edge_attr = zone_ap_graph_creator(X_train[:,:15], y_train) #el grafo lo armo solo con los datos de 2.4Ghz\n",
    "        data = build_heterodata(ap_edge_index, ap_edge_attr, zone_ap_edge_index, zone_ap_edge_attr, num_classes)\n",
    "        T.ToUndirected()(data)\n",
    "\n",
    "        # Armado del dataset\n",
    "\n",
    "        x_training_data = np.reshape(X_train,(X_train.shape[0],15,2))\n",
    "        x_test_data = np.reshape(X_test,(X_test.shape[0],15,2))\n",
    "        y_training_data = y_train\n",
    "        y_test_data = y_test\n",
    "\n",
    "        #normalize (x-mean)/std\n",
    "        mean = x_training_data.mean(axis=0)\n",
    "        std = x_training_data.std(axis=0)\n",
    "\n",
    "        x_training_data = x_training_data - mean\n",
    "        x_training_data /= std\n",
    "        x_test_data = x_test_data - mean\n",
    "        x_test_data /= std   \n",
    "\n",
    "        train_dataset = build_dataset(x_training_data, y_training_data, data)\n",
    "        test_dataset = build_dataset(x_test_data, y_test_data, data)  \n",
    "\n",
    "        model = HeteroGNN_simplified(20, 1, 4).to(device)\n",
    "\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=len(test_dataset), shuffle=False)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=5e-4)\n",
    "        loss = torch.nn.CrossEntropyLoss()\n",
    "        scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.8)\n",
    "\n",
    "        train_loss = []\n",
    "        train_accuracy = []\n",
    "        test_loss = []\n",
    "        test_accuracy = []\n",
    "        best_test_accuracy = 0\n",
    "\n",
    "        m = torch.nn.Softmax(dim=1)\n",
    "\n",
    "        for epoch in range(150):\n",
    "            # print(f\"Epoch: {epoch+1}\")\n",
    "\n",
    "            # TRAIN\n",
    "            model.train()\n",
    "            train_accuracy_epoch = []\n",
    "            train_loss_epoch = []\n",
    "            for data in train_loader:\n",
    "                # import ipdb; ipdb.set_trace()\n",
    "\n",
    "                data = data.to(device)\n",
    "\n",
    "                out = model(data.x_dict, data.edge_index_dict, data.edge_attr_dict) \n",
    "\n",
    "                out_zones = out[\"zones\"].cpu().reshape(out[\"zones\"].cpu().shape[0]//num_classes,num_classes)\n",
    "                loss_result = loss(out_zones.cpu(), data[\"zones\"].y.cpu().type(torch.long))\n",
    "                loss_result.backward()\n",
    "                train_loss_epoch.append(loss_result.detach().cpu())\n",
    "\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                output = m(out_zones)\n",
    "                train_accuracy_epoch.append(accuracy_score(data[\"zones\"].y.cpu().reshape(-1).type(torch.long), np.array(torch.argmax(output.cpu(), axis=1))))\n",
    "\n",
    "            # if scheduler.get_last_lr()[0] > 0.0005:\n",
    "            if (epoch+1)%10 == 0:\n",
    "                scheduler.step()\n",
    "\n",
    "            train_accuracy.append(np.mean(train_accuracy_epoch))\n",
    "            train_loss.append(np.mean(train_loss_epoch))\n",
    "\n",
    "            # VALIDATION\n",
    "            model.eval()\n",
    "            test_accuracy_epoch = []\n",
    "            test_loss_epoch = []\n",
    "            for data in test_loader:\n",
    "\n",
    "                data = data.to(device)\n",
    "                out = model(data.x_dict, data.edge_index_dict, data.edge_attr_dict) \n",
    "                # out_zones = torch.mean(out[\"zones\"].cpu(),1)\n",
    "                # out_zones = out_zones.reshape(out_zones.shape[0]//num_classes,num_classes)\n",
    "                out_zones = out[\"zones\"].cpu().reshape(out[\"zones\"].cpu().shape[0]//num_classes,num_classes)\n",
    "\n",
    "                # out_zones = out.cpu().reshape(out.cpu().shape[0]//num_classes,num_classes)\n",
    "\n",
    "                loss_result = loss(out_zones.cpu(), data[\"zones\"].y.cpu().type(torch.long))        \n",
    "                test_loss_epoch.append(loss_result.detach().cpu())\n",
    "\n",
    "                output = m(out_zones)\n",
    "                test_accuracy_epoch.append(accuracy_score(data[\"zones\"].y.cpu().reshape(-1).type(torch.long), np.array(torch.argmax(output.cpu(), axis=1))))\n",
    "\n",
    "            test_accuracy.append(np.mean(test_accuracy_epoch))\n",
    "            test_loss.append(np.mean(test_loss_epoch))\n",
    "\n",
    "        print(f\"Best Accuracy: Train {np.max(train_accuracy)}, Val {np.max(test_accuracy)}\")\n",
    "        accuracy[str(porc)].append(np.max(test_accuracy))\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Análisis variando cantidad de APs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "APs_MAC_2_4 = ['wifi-dc:a5:f4:43:85:c0',\n",
    "'wifi-dc:a5:f4:43:27:e0',\n",
    "'wifi-f8:4f:57:ab:da:00',\n",
    "'wifi-5c:a4:8a:4c:05:c0',\n",
    "'wifi-1c:1d:86:ce:ef:b0',\n",
    "'wifi-dc:a5:f4:43:79:20',\n",
    "'wifi-c0:7b:bc:36:9e:10',\n",
    "'wifi-1c:1d:86:9f:99:20',\n",
    "'wifi-c0:7b:bc:36:af:40',\n",
    "'wifi-c0:7b:bc:36:af:80',\n",
    "'wifi-1c:1d:86:b6:ac:80',\n",
    "'wifi-dc:a5:f4:43:72:e0',\n",
    "'wifi-f8:4f:57:ab:d8:60',\n",
    "'wifi-dc:a5:f4:43:72:90',\n",
    "'wifi-f8:4f:57:ab:ce:20']\n",
    "\n",
    "APs_MAC_5 = ['wifi-dc:a5:f4:45:85:b0',\n",
    "'wifi-dc:a5:f4:45:27:e0',\n",
    "'wifi-f8:4f:57:ad:d9:60',\n",
    "'wifi-5c:a4:8a:4e:05:30',\n",
    "'wifi-1c:1d:86:d0:ef:00',\n",
    "'wifi-dc:a5:f4:45:79:10',\n",
    "'wifi-c0:7b:bc:38:9e:00',\n",
    "'wifi-1c:1d:86:a1:99:00',\n",
    "'wifi-c0:7b:bc:38:af:30',\n",
    "'wifi-c0:7b:bc:38:af:70',\n",
    "'wifi-1c:1d:86:b8:ac:80',\n",
    "'wifi-dc:a5:f4:45:72:d0',\n",
    "'wifi-f8:4f:57:ad:d7:c0',\n",
    "'wifi-dc:a5:f4:45:72:80',\n",
    "'wifi-f8:4f:57:ad:cd:80']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "batch_size = 8\n",
    "learning_rate = 0.001  \n",
    "print_every = 5\n",
    "\n",
    "aps = ['AP1', 'AP2', 'AP3', 'AP4', 'AP5', 'AP6', 'AP7', 'AP8', 'AP9', 'AP10', 'AP11', 'AP12', 'AP13', 'AP14', 'AP15']\n",
    "APs_to_remove = [12] #, 9, 13, 2, 7] \n",
    "accuracy = {}\n",
    "\n",
    "for ap in APs_to_remove:\n",
    "    print('Nuevo AP removido: ', ap)\n",
    "    APs_MAC_2_4.pop(ap)\n",
    "    APs_MAC_5.pop(ap)\n",
    "    aps.pop(ap)\n",
    "    \n",
    "    # Split de los datos y armado del objeto Data con el grafo\n",
    "    X_train, X_test, y_train, y_test, num_aps, num_classes, enc_train = preprocess_dataset(dataset)\n",
    "    ap_edge_index, ap_edge_attr = ap_graph_creator(X_train[:,:len(aps)], th=10, cols=aps) #el grafo lo armo solo con los datos de 2.4Ghz\n",
    "    zone_ap_edge_index, zone_ap_edge_attr = zone_ap_graph_creator(X_train[:,:len(aps)], y_train, cols=aps) #el grafo lo armo solo con los datos de 2.4Ghz\n",
    "    data = build_heterodata(ap_edge_index, ap_edge_attr, zone_ap_edge_index, zone_ap_edge_attr, num_classes)\n",
    "    T.ToUndirected()(data)\n",
    "    \n",
    "    # Armado del dataset\n",
    "\n",
    "    x_training_data = np.reshape(X_train,(X_train.shape[0],len(aps),2))\n",
    "    x_test_data = np.reshape(X_test,(X_test.shape[0],len(aps),2))\n",
    "    y_training_data = y_train\n",
    "    y_test_data = y_test\n",
    "\n",
    "    #normalize (x-mean)/std\n",
    "    mean = x_training_data.mean(axis=0)\n",
    "    std = x_training_data.std(axis=0)\n",
    "\n",
    "    x_training_data = x_training_data - mean\n",
    "    x_training_data /= std\n",
    "    x_test_data = x_test_data - mean\n",
    "    x_test_data /= std   \n",
    "    \n",
    "    train_dataset = build_dataset(x_training_data, y_training_data, data)\n",
    "    test_dataset = build_dataset(x_test_data, y_test_data, data) \n",
    "  \n",
    "    model = HeteroGNN(20, 1, 4) #.to(device)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=len(test_dataset), shuffle=False)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=5e-4)\n",
    "    loss = torch.nn.CrossEntropyLoss()\n",
    "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.8)\n",
    "    \n",
    "    train_loss = []\n",
    "    train_accuracy = []\n",
    "    test_loss = []\n",
    "    test_accuracy = []\n",
    "    best_test_accuracy = 0\n",
    "\n",
    "    m = torch.nn.Softmax(dim=1)\n",
    "\n",
    "    for epoch in range(100):\n",
    "        # print(f\"Epoch: {epoch+1}\")\n",
    "\n",
    "        # TRAIN\n",
    "        model.train()\n",
    "        train_accuracy_epoch = []\n",
    "        train_loss_epoch = []\n",
    "        for data in train_loader:\n",
    "            import ipdb; ipdb.set_trace()\n",
    "\n",
    "            # data = data.to(device)\n",
    "\n",
    "            out = model(data.x_dict, data.edge_index_dict, data.edge_attr_dict) \n",
    "\n",
    "            out_zones = out[\"zones\"].cpu().reshape(out[\"zones\"].cpu().shape[0]//num_classes,num_classes)\n",
    "            loss_result = loss(out_zones.cpu(), data[\"zones\"].y.cpu().type(torch.long))\n",
    "            loss_result.backward()\n",
    "            train_loss_epoch.append(loss_result.detach().cpu())\n",
    "\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = m(out_zones)\n",
    "            train_accuracy_epoch.append(accuracy_score(data[\"zones\"].y.cpu().reshape(-1).type(torch.long), np.array(torch.argmax(output.cpu(), axis=1))))\n",
    "\n",
    "        # if scheduler.get_last_lr()[0] > 0.0005:\n",
    "        if (epoch+1)%20 == 0:\n",
    "            scheduler.step()\n",
    "\n",
    "        train_accuracy.append(np.mean(train_accuracy_epoch))\n",
    "        train_loss.append(np.mean(train_loss_epoch))\n",
    "\n",
    "        # VALIDATION\n",
    "        model.eval()\n",
    "        test_accuracy_epoch = []\n",
    "        test_loss_epoch = []\n",
    "        for data in test_loader:\n",
    "\n",
    "            data = data.to(device)\n",
    "            out = model(data.x_dict, data.edge_index_dict, data.edge_attr_dict) \n",
    "            # out_zones = torch.mean(out[\"zones\"].cpu(),1)\n",
    "            # out_zones = out_zones.reshape(out_zones.shape[0]//num_classes,num_classes)\n",
    "            out_zones = out[\"zones\"].cpu().reshape(out[\"zones\"].cpu().shape[0]//num_classes,num_classes)\n",
    "\n",
    "            # out_zones = out.cpu().reshape(out.cpu().shape[0]//num_classes,num_classes)\n",
    "\n",
    "            loss_result = loss(out_zones.cpu(), data[\"zones\"].y.cpu().type(torch.long))        \n",
    "            test_loss_epoch.append(loss_result.detach().cpu())\n",
    "\n",
    "            output = m(out_zones)\n",
    "            test_accuracy_epoch.append(accuracy_score(data[\"zones\"].y.cpu().reshape(-1).type(torch.long), np.array(torch.argmax(output.cpu(), axis=1))))\n",
    "\n",
    "        test_accuracy.append(np.mean(test_accuracy_epoch))\n",
    "        test_loss.append(np.mean(test_loss_epoch))\n",
    "\n",
    "    print(f\"Best Accuracy: Train {np.max(train_accuracy)}, Val {np.max(test_accuracy)}\")\n",
    "    accuracy[str(porc)] = np.max(test_accuracy)\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(train_loss, label=\"Train loss\")\n",
    "    plt.plot(test_loss, label=\"Validation loss\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(train_accuracy, label=\"Train accuracy\")\n",
    "    plt.plot(test_accuracy, label=\"Validation accuracy\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()    \n",
    "\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Análisis de APs caídos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entreno normalmente y después testeo con APs caídos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "lst = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14]\n",
    "accuracy_ap_caido_gnn = {\"1\":[], \"2\":[], \"3\":[]}\n",
    "\n",
    "m = torch.nn.Softmax(dim=1)\n",
    "\n",
    "for ap in [1,2,3]:\n",
    "    print(\"Cantidad de APs caídos: \", ap)\n",
    "\n",
    "    for combo in combinations(lst, ap):  # 2 for pairs, 3 for triplets, etc\n",
    "        # Split de los datos y armado del objeto Data con el grafo\n",
    "\n",
    "        X_train, X_test, y_train, y_test, num_aps, num_classes, enc_train = preprocess_dataset(dataset)\n",
    "        ap_edge_index, ap_edge_attr = ap_graph_creator(X_train[:,:15], th=10, prune_th=10) #el grafo lo armo solo con los datos de 2.4Ghz\n",
    "        zone_ap_edge_index, zone_ap_edge_attr = zone_ap_graph_creator(X_train[:,:15], y_train, prune_th=10) #el grafo lo armo solo con los datos de 2.4Ghz\n",
    "        data = build_heterodata(ap_edge_index, ap_edge_attr, zone_ap_edge_index, zone_ap_edge_attr)\n",
    "        T.ToUndirected()(data)\n",
    "\n",
    "        # Armado del dataset\n",
    "\n",
    "        x_training_data = np.reshape(X_train,(X_train.shape[0],15,2))\n",
    "        x_test_data = np.reshape(X_test,(X_test.shape[0],15,2))\n",
    "        y_training_data = y_train\n",
    "        y_test_data = y_test\n",
    "\n",
    "        #normalize (x-mean)/std\n",
    "        mean = x_training_data.mean(axis=0)\n",
    "        std = x_training_data.std(axis=0)\n",
    "\n",
    "        x_training_data = x_training_data - mean\n",
    "        x_training_data /= std\n",
    "        x_test_data = x_test_data - mean\n",
    "        x_test_data /= std     \n",
    "\n",
    "        for i in combo:        \n",
    "            x_test_data[:,i] = 0\n",
    "\n",
    "        test_dataset = build_dataset(x_test_data, y_test_data, data)     \n",
    "\n",
    "        model = HeteroGNN(20, 1, 4)\n",
    "        model.load_state_dict(torch.load(\"MNAV_HeteroGNN_best_model_968.pth\"))\n",
    "        model.to(device)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=len(test_dataset), shuffle=False)  \n",
    "\n",
    "        # TEST\n",
    "        model.eval()\n",
    "        for data in test_loader:\n",
    "            data = data.to(device)\n",
    "            out = model(data.x_dict, data.edge_index_dict, data.edge_attr_dict) \n",
    "            out_zones = out[\"zones\"].cpu().reshape(out[\"zones\"].cpu().shape[0]//num_classes,num_classes)\n",
    "\n",
    "            output = m(out_zones)\n",
    "        accuracy_ap_caido_gnn[str(ap)].append(accuracy_score(data[\"zones\"].y.cpu().reshape(-1).type(torch.long), np.array(torch.argmax(output.cpu(), axis=1))))\n",
    "        print(accuracy_ap_caido_gnn[str(ap)][-1])\n",
    "    print(f\"    Accuracy promedio: {np.mean(accuracy_ap_caido_gnn[str(ap)])}\")\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_ap_caido_gnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ap in [1,2,3]:\n",
    "    print(f\"{ap}: {np.mean(accuracy_ap_caido_gnn[str(ap)])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14]\n",
    "X_train, X_test, y_train, y_test, num_aps, num_classes, enc_train = preprocess_dataset(dataset)\n",
    "\n",
    "neigh = KNeighborsClassifier(n_neighbors=3)\n",
    "neigh.fit(X_train, y_train)\n",
    "\n",
    "accuracy_ap_caido_knn = {\"1\":[], \"2\":[], \"3\":[]}\n",
    "\n",
    "for ap in [1,2,3]:\n",
    "    print(\"Cantidad de APs caídos: \", ap)\n",
    "    for combo in combinations(lst, ap):  # 2 for pairs, 3 for triplets, etc\n",
    "        X_aux_test = deepcopy(X_test)\n",
    "        for i in combo:\n",
    "            X_aux_test[:,i] = 0\n",
    "            X_aux_test[:,i+15] = 0\n",
    "\n",
    "        y_pred_knn = neigh.predict(X_aux_test)\n",
    "\n",
    "        ACC_test = accuracy_score(y_test, y_pred_knn)\n",
    "        accuracy_ap_caido_knn[str(ap)].append(ACC_test)\n",
    "    print(\"    Accuracy: \", np.mean(accuracy_ap_caido_knn[str(ap)]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_ap_caido_knn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Análisis de obstáculos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:1')\n",
    "\n",
    "lst = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14]\n",
    "obstaculo_gnn_hetero = {\"5\":[], \"10\":[], \"15\":[]}\n",
    "obstaculos = [-5] #[-10, -15] # dB\n",
    "\n",
    "m = torch.nn.Softmax(dim=1)\n",
    "\n",
    "for obstaculo in obstaculos:\n",
    "    print(f\"Obstáculo: {obstaculo}\")\n",
    "\n",
    "    for ap in lst:\n",
    "        \n",
    "        # Split de los datos y armado del objeto Data con el grafo        \n",
    "        X_train, X_test, y_train, y_test, num_aps, num_classes, enc_train = preprocess_dataset(dataset)\n",
    "        ap_edge_index, ap_edge_attr = ap_graph_creator(X_train[:,:15], th=10, prune_th=10) #el grafo lo armo solo con los datos de 2.4Ghz\n",
    "        zone_ap_edge_index, zone_ap_edge_attr = zone_ap_graph_creator(X_train[:,:15], y_train, prune_th=10) #el grafo lo armo solo con los datos de 2.4Ghz\n",
    "        data = build_heterodata(ap_edge_index, ap_edge_attr, zone_ap_edge_index, zone_ap_edge_attr)\n",
    "        T.ToUndirected()(data)\n",
    "        \n",
    "        # Armado del dataset\n",
    "\n",
    "        x_training_data = np.reshape(X_train,(X_train.shape[0],15,2))\n",
    "        x_test_data = np.reshape(X_test,(X_test.shape[0],15,2))\n",
    "        y_training_data = y_train\n",
    "        y_test_data = y_test\n",
    "\n",
    "        x_test_data[:,ap] += obstaculo\n",
    "        \n",
    "        #normalize (x-mean)/std\n",
    "        mean = x_training_data.mean(axis=0)\n",
    "        std = x_training_data.std(axis=0)\n",
    "\n",
    "        x_training_data = x_training_data - mean\n",
    "        x_training_data /= std\n",
    "        x_test_data = x_test_data - mean\n",
    "        x_test_data /= std     \n",
    "\n",
    "        test_dataset = build_dataset(x_test_data, y_test_data, data)     \n",
    "\n",
    "        model = HeteroGNN(20, 1, 4)\n",
    "        model.load_state_dict(torch.load(\"MNAV_HeteroGNN_best_model_968.pth\"))\n",
    "        model.to(device)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=len(test_dataset), shuffle=False)  \n",
    "\n",
    "        # TEST\n",
    "        model.eval()\n",
    "        for data in test_loader:\n",
    "            data = data.to(device)\n",
    "            out = model(data.x_dict, data.edge_index_dict, data.edge_attr_dict) \n",
    "            out_zones = out[\"zones\"].cpu().reshape(out[\"zones\"].cpu().shape[0]//num_classes,num_classes)\n",
    "\n",
    "            output = m(out_zones)\n",
    "        obstaculo_gnn_hetero[str(abs(obstaculo))].append(accuracy_score(data[\"zones\"].y.cpu().reshape(-1).type(torch.long), np.array(torch.argmax(output.cpu(), axis=1))))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obstaculo_gnn_hetero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cambio en GSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:1')\n",
    "\n",
    "lst = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14]\n",
    "obstaculo_gnn_hetero_gso = {\"5\":[], \"10\":[], \"15\":[]}\n",
    "obstaculos = [-5] # [-10, -15] # dB\n",
    "\n",
    "m = torch.nn.Softmax(dim=1)\n",
    "\n",
    "for obstaculo in obstaculos:\n",
    "    print(f\"Obstáculo: {obstaculo}\")\n",
    "\n",
    "    for ap in lst:\n",
    "        \n",
    "        # Split de los datos y armado del objeto Data con el grafo        \n",
    "        X_train, X_test, y_train, y_test, num_aps, num_classes, enc_train = preprocess_dataset(dataset)\n",
    "\n",
    "        # Build graph with modified fingerprints\n",
    "        X_train[:,ap] += obstaculo\n",
    "        X_train[:,ap+15] += obstaculo\n",
    "        \n",
    "        ap_edge_index, ap_edge_attr = ap_graph_creator(X_train[:,:15], th=10, prune_th=10) #el grafo lo armo solo con los datos de 2.4Ghz\n",
    "        zone_ap_edge_index, zone_ap_edge_attr = zone_ap_graph_creator(X_train[:,:15], y_train, prune_th=10) #el grafo lo armo solo con los datos de 2.4Ghz\n",
    "        data = build_heterodata(ap_edge_index, ap_edge_attr, zone_ap_edge_index, zone_ap_edge_attr)\n",
    "        T.ToUndirected()(data)\n",
    "        \n",
    "        # Armado del dataset\n",
    "\n",
    "        x_training_data = np.reshape(X_train,(X_train.shape[0],15,2))\n",
    "        x_test_data = np.reshape(X_test,(X_test.shape[0],15,2))\n",
    "        y_training_data = y_train\n",
    "        y_test_data = y_test\n",
    "\n",
    "        x_test_data[:,ap] += obstaculo\n",
    "        \n",
    "        #normalize (x-mean)/std\n",
    "        mean = x_training_data.mean(axis=0)\n",
    "        std = x_training_data.std(axis=0)\n",
    "\n",
    "        x_training_data = x_training_data - mean\n",
    "        x_training_data /= std\n",
    "        x_test_data = x_test_data - mean\n",
    "        x_test_data /= std     \n",
    "\n",
    "        test_dataset = build_dataset(x_test_data, y_test_data, data)     \n",
    "\n",
    "        model = HeteroGNN(20, 1, 4)\n",
    "        model.load_state_dict(torch.load(\"MNAV_HeteroGNN_best_model_968.pth\"))\n",
    "        model.to(device)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=len(test_dataset), shuffle=False)  \n",
    "\n",
    "        # TEST\n",
    "        model.eval()\n",
    "        for data in test_loader:\n",
    "            data = data.to(device)\n",
    "            out = model(data.x_dict, data.edge_index_dict, data.edge_attr_dict) \n",
    "            out_zones = out[\"zones\"].cpu().reshape(out[\"zones\"].cpu().shape[0]//num_classes,num_classes)\n",
    "\n",
    "            output = m(out_zones)\n",
    "        obstaculo_gnn_hetero_gso[str(abs(obstaculo))].append(accuracy_score(data[\"zones\"].y.cpu().reshape(-1).type(torch.long), np.array(torch.argmax(output.cpu(), axis=1))))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obstaculo_gnn_hetero_gso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obstaculo_gnn = {\"10\":[], \"15\":[]}\n",
    "obstaculo_gnn[\"10\"] = [0.9302769818529131, 0.9441260744985673, 0.9498567335243553, 0.938872970391595, 0.9379178605539638, 0.9336198662846227, 0.8720152817574021, 0.9326647564469914, 0.8667621776504298, 0.9274116523400191, 0.9274116523400191, 0.9383954154727794, 0.8046800382043935, 0.9450811843361987, 0.9097421203438395]\n",
    "obstaculo_gnn[\"15\"] = [0.887774594078319, 0.9240687679083095, 0.9422158548233047, 0.9149952244508118, 0.8982808022922636, 0.9097421203438395, 0.839541547277937, 0.899235912129895, 0.8166189111747851, 0.9063992359121299, 0.8863419293218721, 0.9250238777459407, 0.670487106017192, 0.9259789875835721, 0.8443170964660937]\n",
    "\n",
    "obstaculo_knn = {\"10\":[], \"15\":[]}\n",
    "obstaculo_knn[\"10\"].append(0.9493791786055397)\n",
    "obstaculo_knn[\"15\"].append(0.9350525310410697)\n",
    "\n",
    "obstaculo_knn[\"10\"].append(0.944603629417383)\n",
    "obstaculo_knn[\"15\"].append(0.9369627507163324)\n",
    "\n",
    "obstaculo_knn[\"10\"].append(0.9479465138490927)\n",
    "obstaculo_knn[\"15\"].append(0.9288443170964661)\n",
    "\n",
    "obstaculo_knn[\"10\"].append(0.9412607449856734)\n",
    "obstaculo_knn[\"15\"].append(0.9202483285577842)\n",
    "\n",
    "obstaculo_knn[\"10\"].append(0.941738299904489)\n",
    "obstaculo_knn[\"15\"].append(0.9216809933142311)\n",
    "\n",
    "obstaculo_knn[\"10\"].append(0.9364851957975168)\n",
    "obstaculo_knn[\"15\"].append(0.9106972301814709)\n",
    "\n",
    "obstaculo_knn[\"10\"].append(0.9044890162368673)\n",
    "obstaculo_knn[\"15\"].append(0.8419293218720153)\n",
    "\n",
    "obstaculo_knn[\"10\"].append(0.9293218720152817)\n",
    "obstaculo_knn[\"15\"].append(0.89207258834766)\n",
    "\n",
    "obstaculo_knn[\"10\"].append(0.9063992359121299)\n",
    "obstaculo_knn[\"15\"].append(0.8462273161413563)\n",
    "\n",
    "obstaculo_knn[\"10\"].append(0.9278892072588347)\n",
    "obstaculo_knn[\"15\"].append(0.8954154727793696)\n",
    "\n",
    "obstaculo_knn[\"10\"].append(0.9293218720152817)\n",
    "obstaculo_knn[\"15\"].append(0.8810888252148997)\n",
    "\n",
    "obstaculo_knn[\"10\"].append(0.9240687679083095)\n",
    "obstaculo_knn[\"15\"].append(0.8720152817574021)\n",
    "\n",
    "obstaculo_knn[\"10\"].append(0.9493791786055397)\n",
    "obstaculo_knn[\"15\"].append(0.9383954154727794)\n",
    "\n",
    "obstaculo_knn[\"10\"].append(0.9498567335243553)\n",
    "obstaculo_knn[\"15\"].append(0.9336198662846227)\n",
    "\n",
    "obstaculo_knn[\"10\"].append(0.9551098376313276)\n",
    "obstaculo_knn[\"15\"].append(0.9484240687679083)\n",
    "\n",
    "obstaculo_fcnn = {\"10\":[], \"15\":[]}\n",
    "\n",
    "obstaculo_fcnn[\"10\"].append(0.9188156638013372)\n",
    "obstaculo_fcnn[\"15\"].append(0.8533906399235912)\n",
    "obstaculo_fcnn[\"10\"].append(0.8767908309455588)\n",
    "obstaculo_fcnn[\"15\"].append(0.791308500477555)\n",
    "obstaculo_fcnn[\"10\"].append(0.8524355300859598)\n",
    "obstaculo_fcnn[\"15\"].append(0.7717287488061128)\n",
    "obstaculo_fcnn[\"10\"].append(0.8901623686723973)\n",
    "obstaculo_fcnn[\"15\"].append(0.8223495702005731)\n",
    "obstaculo_fcnn[\"10\"].append(0.8299904489016237)\n",
    "obstaculo_fcnn[\"15\"].append(0.7311365807067812)\n",
    "obstaculo_fcnn[\"10\"].append(0.8237822349570201)\n",
    "obstaculo_fcnn[\"15\"].append(0.7120343839541547)\n",
    "obstaculo_fcnn[\"10\"].append(0.7917860553963706)\n",
    "obstaculo_fcnn[\"15\"].append(0.6614135625596944)\n",
    "obstaculo_fcnn[\"10\"].append(0.9288443170964661)\n",
    "obstaculo_fcnn[\"15\"].append(0.8810888252148997)\n",
    "obstaculo_fcnn[\"10\"].append(0.8022922636103151)\n",
    "obstaculo_fcnn[\"15\"].append(0.6466093600764088)\n",
    "obstaculo_fcnn[\"10\"].append(0.8915950334288443)\n",
    "obstaculo_fcnn[\"15\"].append(0.7893982808022922)\n",
    "obstaculo_fcnn[\"10\"].append(0.893505253104107)\n",
    "obstaculo_fcnn[\"15\"].append(0.8166189111747851)\n",
    "obstaculo_fcnn[\"10\"].append(0.9001910219675263)\n",
    "obstaculo_fcnn[\"15\"].append(0.8314231136580706)\n",
    "obstaculo_fcnn[\"10\"].append(0.7717287488061128)\n",
    "obstaculo_fcnn[\"15\"].append(0.663323782234957)\n",
    "obstaculo_fcnn[\"10\"].append(0.779847182425979)\n",
    "obstaculo_fcnn[\"15\"].append(0.667621776504298)\n",
    "obstaculo_fcnn[\"10\"].append(0.7516714422158548)\n",
    "obstaculo_fcnn[\"15\"].append(0.6365807067812799)\n",
    "\n",
    "obstaculo_bracco = {\"10\":[], \"15\":[]}\n",
    "\n",
    "obstaculo_bracco[\"10\"].append(0.9584527220630372)\n",
    "obstaculo_bracco[\"15\"].append(0.9489016236867239)\n",
    "obstaculo_bracco[\"10\"].append(0.9613180515759312)\n",
    "obstaculo_bracco[\"15\"].append(0.9527220630372493)\n",
    "obstaculo_bracco[\"10\"].append(0.9489016236867239)\n",
    "obstaculo_bracco[\"15\"].append(0.9321872015281757)\n",
    "obstaculo_bracco[\"10\"].append(0.9474689589302769)\n",
    "obstaculo_bracco[\"15\"].append(0.9245463228271251)\n",
    "obstaculo_bracco[\"10\"].append(0.9355300859598854)\n",
    "obstaculo_bracco[\"15\"].append(0.9044890162368673)\n",
    "obstaculo_bracco[\"10\"].append(0.9221585482330468)\n",
    "obstaculo_bracco[\"15\"].append(0.8787010506208214)\n",
    "obstaculo_bracco[\"10\"].append(0.8724928366762178)\n",
    "obstaculo_bracco[\"15\"].append(0.779847182425979)\n",
    "obstaculo_bracco[\"10\"].append(0.9469914040114613)\n",
    "obstaculo_bracco[\"15\"].append(0.9102196752626552)\n",
    "obstaculo_bracco[\"10\"].append(0.8801337153772684)\n",
    "obstaculo_bracco[\"15\"].append(0.7774594078319007)\n",
    "obstaculo_bracco[\"10\"].append(0.9522445081184336)\n",
    "obstaculo_bracco[\"15\"].append(0.9283667621776505)\n",
    "obstaculo_bracco[\"10\"].append(0.9383954154727794)\n",
    "obstaculo_bracco[\"15\"].append(0.8925501432664756)\n",
    "obstaculo_bracco[\"10\"].append(0.9216809933142311)\n",
    "obstaculo_bracco[\"15\"].append(0.8619866284622731)\n",
    "obstaculo_bracco[\"10\"].append(0.9527220630372493)\n",
    "obstaculo_bracco[\"15\"].append(0.9340974212034384)\n",
    "obstaculo_bracco[\"10\"].append(0.9398280802292264)\n",
    "obstaculo_bracco[\"15\"].append(0.9087870105062082)\n",
    "obstaculo_bracco[\"10\"].append(0.9613180515759312)\n",
    "obstaculo_bracco[\"15\"].append(0.9522445081184336)\n",
    "\n",
    "obstaculo_gnn_gso = {\"10\":[], \"15\":[]}\n",
    "\n",
    "obstaculo_gnn_gso[\"10\"].append(0.937440305635148)\n",
    "obstaculo_gnn_gso[\"15\"].append(0.9154727793696275)\n",
    "obstaculo_gnn_gso[\"10\"].append(0.944603629417383)\n",
    "obstaculo_gnn_gso[\"15\"].append(0.9336198662846227)\n",
    "obstaculo_gnn_gso[\"10\"].append(0.9536771728748806)\n",
    "obstaculo_gnn_gso[\"15\"].append(0.9512893982808023)\n",
    "obstaculo_gnn_gso[\"10\"].append(0.933142311365807)\n",
    "obstaculo_gnn_gso[\"15\"].append(0.9049665711556829)\n",
    "obstaculo_gnn_gso[\"10\"].append(0.9441260744985673)\n",
    "obstaculo_gnn_gso[\"15\"].append(0.8987583572110793)\n",
    "obstaculo_gnn_gso[\"10\"].append(0.9412607449856734)\n",
    "obstaculo_gnn_gso[\"15\"].append(0.9321872015281757)\n",
    "obstaculo_gnn_gso[\"10\"].append(0.8820439350525311)\n",
    "obstaculo_gnn_gso[\"15\"].append(0.8510028653295129)\n",
    "obstaculo_gnn_gso[\"10\"].append(0.9336198662846227)\n",
    "obstaculo_gnn_gso[\"15\"].append(0.8958930276981852)\n",
    "obstaculo_gnn_gso[\"10\"].append(0.8629417382999045)\n",
    "obstaculo_gnn_gso[\"15\"].append(0.8218720152817574)\n",
    "obstaculo_gnn_gso[\"10\"].append(0.9274116523400191)\n",
    "obstaculo_gnn_gso[\"15\"].append(0.9016236867239733)\n",
    "obstaculo_gnn_gso[\"10\"].append(0.9269340974212035)\n",
    "obstaculo_gnn_gso[\"15\"].append(0.890639923591213)\n",
    "obstaculo_gnn_gso[\"10\"].append(0.937440305635148)\n",
    "obstaculo_gnn_gso[\"15\"].append(0.9226361031518625)\n",
    "obstaculo_gnn_gso[\"10\"].append(0.826647564469914)\n",
    "obstaculo_gnn_gso[\"15\"].append(0.7048710601719198)\n",
    "obstaculo_gnn_gso[\"10\"].append(0.9508118433619867)\n",
    "obstaculo_gnn_gso[\"15\"].append(0.9307545367717287)\n",
    "obstaculo_gnn_gso[\"10\"].append(0.9097421203438395)\n",
    "obstaculo_gnn_gso[\"15\"].append(0.8443170964660937)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"GNN 10: {np.mean(obstaculo_gnn['10'])}\")\n",
    "print(f\"GNN 15: {np.mean(obstaculo_gnn['15'])}\")\n",
    "print(f\"KNN 10: {np.mean(obstaculo_knn['10'])}\")\n",
    "print(f\"KNN 15: {np.mean(obstaculo_knn['15'])}\")\n",
    "print(f\"FCNN 10: {np.mean(obstaculo_fcnn['10'])}\")\n",
    "print(f\"FCNN 15: {np.mean(obstaculo_fcnn['15'])}\")\n",
    "print(f\"BRACCO 10: {np.mean(obstaculo_bracco['10'])}\")\n",
    "print(f\"BRACCO 15: {np.mean(obstaculo_bracco['15'])}\")\n",
    "print(f\"GNN GSO 10: {np.mean(obstaculo_gnn_gso['10'])}\")\n",
    "print(f\"GNN GSO 15: {np.mean(obstaculo_gnn_gso['15'])}\")\n",
    "print(f\"GNN Hetero 10: {np.mean(obstaculo_gnn_hetero['10'])}\")\n",
    "print(f\"GNN Hetero 15: {np.mean(obstaculo_gnn_hetero['15'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "neigh = KNeighborsClassifier(n_neighbors=3)\n",
    "neigh.fit(X_train, y_train)\n",
    "y_pred_knn = neigh.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred_knn))\n",
    "print(classification_report(y_test, y_pred_knn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## FCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "clf = MLPClassifier().fit(X_train, y_train)\n",
    "y_pred_fcnn = clf.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred_fcnn))\n",
    "print(classification_report(y_test, y_pred_fcnn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "batch_size = 16\n",
    "learning_rate = 0.01 \n",
    "print_every = 50\n",
    "\n",
    "prune_thresholds = [20]\n",
    "accuracy = {\"20\":[]}\n",
    "\n",
    "for prune_th in prune_thresholds:\n",
    "    print('Threshold: ', prune_th)\n",
    "        \n",
    "    for _ in range(10):\n",
    "        X_train, X_test, y_train, y_test, num_aps, num_classes, enc_train = preprocess_dataset(dataset, dataset_percentage=0.3)\n",
    "        ap_edge_index, ap_edge_attr = ap_graph_creator(X_train[:,:15], th=20, prune_th=prune_th) #el grafo lo armo solo con los datos de 2.4Ghz\n",
    "        zone_ap_edge_index, zone_ap_edge_attr = zone_ap_graph_creator(X_train[:,:15], y_train, prune_th=prune_th) #el grafo lo armo solo con los datos de 2.4Ghz\n",
    "        data = build_heterodata(ap_edge_index, ap_edge_attr, zone_ap_edge_index, zone_ap_edge_attr)\n",
    "        T.ToUndirected()(data)\n",
    "\n",
    "        # Armado del dataset\n",
    "\n",
    "        x_training_data = np.reshape(X_train,(X_train.shape[0],15,2))\n",
    "        x_test_data = np.reshape(X_test,(X_test.shape[0],15,2))\n",
    "        y_training_data = y_train\n",
    "        y_test_data = y_test\n",
    "\n",
    "        #normalize (x-mean)/std\n",
    "        mean = x_training_data.mean(axis=0)\n",
    "        std = x_training_data.std(axis=0)\n",
    "\n",
    "        x_training_data = x_training_data - mean\n",
    "        x_training_data /= std\n",
    "        x_test_data = x_test_data - mean\n",
    "        x_test_data /= std \n",
    "\n",
    "        train_dataset = build_dataset(x_training_data, y_training_data, data)\n",
    "        test_dataset = build_dataset(x_test_data, y_test_data, data) \n",
    "    \n",
    "        model = HeteroGNN(20, 1, 4).to(device)\n",
    "\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=len(test_dataset), shuffle=False)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=5e-4)\n",
    "        loss = torch.nn.CrossEntropyLoss()\n",
    "        scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.8)\n",
    "\n",
    "        train_loss = []\n",
    "        train_accuracy = []\n",
    "        test_loss = []\n",
    "        test_accuracy = []\n",
    "        best_test_accuracy = 0\n",
    "\n",
    "\n",
    "        m = torch.nn.Softmax(dim=1)\n",
    "\n",
    "        for epoch in range(150):\n",
    "            # print(f\"Epoch: {epoch+1}\")\n",
    "\n",
    "            # TRAIN\n",
    "            model.train()\n",
    "            train_accuracy_epoch = []\n",
    "            train_loss_epoch = []\n",
    "            for data in train_loader:\n",
    "\n",
    "                data = data.to(device)\n",
    "                out = model(data.x_dict, data.edge_index_dict, data.edge_attr_dict) \n",
    "                out_zones = out[\"zones\"].cpu().reshape(out[\"zones\"].cpu().shape[0]//num_classes,num_classes)\n",
    "\n",
    "                loss_result = loss(out_zones.cpu(), data[\"zones\"].y.cpu().type(torch.long))\n",
    "                loss_result.backward()\n",
    "                train_loss_epoch.append(loss_result.detach().cpu())\n",
    "\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                output = m(out_zones)\n",
    "                train_accuracy_epoch.append(accuracy_score(data[\"zones\"].y.cpu().reshape(-1).type(torch.long), np.array(torch.argmax(output.cpu(), axis=1))))\n",
    "\n",
    "            # if scheduler.get_last_lr()[0] > 0.0005:\n",
    "            if (epoch+1)%20 == 0:\n",
    "                scheduler.step()\n",
    "\n",
    "            train_accuracy.append(np.mean(train_accuracy_epoch))\n",
    "            train_loss.append(np.mean(train_loss_epoch))\n",
    "\n",
    "\n",
    "\n",
    "            # VALIDATION\n",
    "            model.eval()\n",
    "            test_accuracy_epoch = []\n",
    "            test_loss_epoch = []\n",
    "            for data in test_loader:\n",
    "\n",
    "                data = data.to(device)\n",
    "                out = model(data.x_dict, data.edge_index_dict, data.edge_attr_dict) \n",
    "                out_zones = out[\"zones\"].cpu().reshape(out[\"zones\"].cpu().shape[0]//num_classes,num_classes)\n",
    "                loss_result = loss(out_zones.cpu(), data[\"zones\"].y.cpu().type(torch.long))        \n",
    "                test_loss_epoch.append(loss_result.detach().cpu())\n",
    "\n",
    "                output = m(out_zones)\n",
    "                test_accuracy_epoch.append(accuracy_score(data[\"zones\"].y.cpu().reshape(-1).type(torch.long), np.array(torch.argmax(output.cpu(), axis=1))))\n",
    "\n",
    "            test_accuracy.append(np.mean(test_accuracy_epoch))\n",
    "            if test_accuracy[-1] > best_test_accuracy:\n",
    "                best_test_accuracy = test_accuracy[-1]        \n",
    "                torch.save(model.state_dict(), \"MNAV_HeteroGNN_best_model.pth\")\n",
    "\n",
    "            test_loss.append(np.mean(test_loss_epoch))\n",
    "\n",
    "            # if (epoch+1)%print_every == 0:\n",
    "            #     print(f\"Epoch {epoch+1}, Train Loss {np.mean(train_loss_epoch)}, Val Loss {np.mean(test_loss_epoch)}\")\n",
    "\n",
    "        print(f\"Last LR: {scheduler.get_last_lr()}\")\n",
    "        print(f\"Best Accuracy: Train {np.max(train_accuracy)}, Val {np.max(test_accuracy)}\") \n",
    "    \n",
    "        accuracy[str(prune_th)].append(np.max(test_accuracy))\n",
    "\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_prunning = accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_prunning = {\"20\": [0.9268680445151033, 0.9252782193958664, 0.9252782193958664, 0.931637519872814, 0.9332273449920508, 0.9220985691573926, 0.9268680445151033, 0.9475357710651828, 0.9427662957074722, 0.9125596184419714]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(accuracy_prunning[\"20\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux = []\n",
    "aux.append(accuracy_prunning['20'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[9,7])\n",
    "plt.boxplot(aux, showfliers=False) #, meanline=True, showmeans=True)\n",
    "plt.xticks([1], [30])\n",
    "plt.yticks([0.98, 0.96, 0.94, 0.92, 0.90])\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Fingerprints sample size (%)')\n",
    "plt.title('HeteroGNN prune_th=20')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Pruebas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Explainability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_heterodata_unnormalized(ap_edge_index, ap_edge_attr, zone_ap_edge_index, zone_ap_edge_attr, classes=16):\n",
    "    data = HeteroData()\n",
    "    data['aps', 'ap_ap', 'aps'].edge_index = ap_edge_index\n",
    "    # ap_edge_attr = (ap_edge_attr - ap_edge_attr.mean())/ap_edge_attr.std()\n",
    "    data['aps', 'ap_ap', 'aps'].edge_attr = ap_edge_attr\n",
    "    data['aps'].num_nodes = 15\n",
    "\n",
    "    data['zones', 'zone_ap', 'aps'].edge_index = zone_ap_edge_index\n",
    "    # zone_ap_edge_attr = (zone_ap_edge_attr - zone_ap_edge_attr.mean())/zone_ap_edge_attr.std()    \n",
    "    # data['zones', 'zone_ap', 'aps'].edge_attr = torch.nn.functional.normalize(zone_ap_edge_attr, dim=0)\n",
    "    data['zones', 'zone_ap', 'aps'].edge_attr = zone_ap_edge_attr\n",
    "    data['zones'].num_nodes = classes\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split de los datos y armado del objeto Data con el grafo\n",
    "\n",
    "X_train, X_test, y_train, y_test, num_aps, num_classes, enc_train = preprocess_dataset(dataset)\n",
    "ap_edge_index, ap_edge_attr = ap_graph_creator(X_train[:,:15], th=10) #el grafo lo armo solo con los datos de 2.4Ghz\n",
    "zone_ap_edge_index, zone_ap_edge_attr = zone_ap_graph_creator(X_train[:,:15], y_train) #el grafo lo armo solo con los datos de 2.4Ghz\n",
    "data = build_heterodata_unnormalized(ap_edge_index, ap_edge_attr, zone_ap_edge_index, zone_ap_edge_attr)\n",
    "T.ToUndirected()(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data\n",
    "# data._edge_store_dict[('zones','zone_ap','aps')]\n",
    "edge_index = data._edge_store_dict[('zones','zone_ap','aps')][\"edge_index\"]\n",
    "edge_attr = data._edge_store_dict[('zones','zone_ap','aps')][\"edge_attr\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(edge_attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = np.zeros((16, 15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "# for zone in range(16):\n",
    "    # for ap in range(15):\n",
    "for i in range(len(edge_attr)):\n",
    "    zone = edge_index[0][i]\n",
    "    ap = edge_index[1][i]\n",
    "    matrix[zone,ap] = edge_attr[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[12,12])\n",
    "ax = sns.heatmap(matrix, linewidth=0.5, cmap=\"YlGnBu\", vmin=10, vmax=35, annot=True)\n",
    "# plt.show()\n",
    "plt.savefig(\"analisis_sacando_zona.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0: 'location_1', \n",
    "# 1: 'location_10', \n",
    "# 2: 'location_11', \n",
    "# 3: 'location_12',\n",
    "# 4: 'location_13', \n",
    "# 5: 'location_14', \n",
    "# 6: 'location_15', \n",
    "# 7: 'location_16',\n",
    "# 8: 'location_2', \n",
    "# 9: 'location_3', \n",
    "# 10: 'location_4', \n",
    "# 11: 'location_5',\n",
    "# 12: 'location_6', \n",
    "# 13: 'location_7', \n",
    "# 14: 'location_8', \n",
    "# 15: 'location_9'],"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index_aps = data._edge_store_dict[('aps','ap_ap','aps')][\"edge_index\"]\n",
    "edge_attr_aps = data._edge_store_dict[('aps','ap_ap','aps')][\"edge_attr\"]\n",
    "matrix_aps = np.zeros((15, 15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "for i in range(len(edge_attr_aps)):\n",
    "    ap1 = edge_index_aps[0][i]\n",
    "    ap2 = edge_index_aps[1][i]\n",
    "    matrix_aps[ap1,ap2] = edge_attr_aps[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[12,11])\n",
    "ax_aps = sns.heatmap(matrix_aps, linewidth=0.5, cmap=\"YlGnBu\", vmin=20, vmax=80, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Simulacion agregando zona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_graph = HeteroData()\n",
    "train_graph['aps', 'ap_ap', 'aps'].edge_index = torch.tensor([[0, 0, 0, 1, 1, 2], [1, 2, 3, 2, 3, 3]])\n",
    "train_graph['aps', 'ap_ap', 'aps'].edge_attr = torch.tensor([[1], [1], [1], [1], [1], [1]])\n",
    "train_graph['aps'].num_nodes = 4\n",
    "\n",
    "train_graph['zones', 'zone_ap', 'aps'].edge_index = torch.tensor([[0, 0, 0, 0, 1, 1], [0, 1, 2, 3, 1, 2]])\n",
    "train_graph['zones', 'zone_ap', 'aps'].edge_attr = torch.tensor([[2], [5], [5], [1], [1], [5]])\n",
    "# train_graph['zones', 'zone_ap', 'aps'].edge_index = torch.tensor([[0, 0, 0, 0, 1, 1, 2, 2, 2], [0, 1, 2, 3, 1, 2, 0, 2, 3]])\n",
    "# train_graph['zones', 'zone_ap', 'aps'].edge_attr = torch.tensor([[2], [5], [5], [1], [1], [5], [0], [0], [0]])\n",
    "train_graph['zones'].num_nodes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = HeteroData()\n",
    "graph['aps', 'ap_ap', 'aps'].edge_index = torch.tensor([[0, 0, 0, 1, 1, 2], [1, 2, 3, 2, 3, 3]])\n",
    "graph['aps', 'ap_ap', 'aps'].edge_attr = torch.tensor([[1], [1], [1], [1], [1], [1]])\n",
    "graph['aps'].num_nodes = 4\n",
    "\n",
    "graph['zones', 'zone_ap', 'aps'].edge_index = torch.tensor([[0, 0, 0, 0, 1, 1, 2, 2, 2], [0, 1, 2, 3, 1, 2, 0, 2, 3]])\n",
    "graph['zones', 'zone_ap', 'aps'].edge_attr = torch.tensor([[2], [5], [5], [1], [1], [5], [2], [5], [5]])\n",
    "graph['zones'].num_nodes = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T.ToUndirected()(train_graph)\n",
    "T.ToUndirected()(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "X_test = []\n",
    "y_train = []\n",
    "y_test = []\n",
    "x_0_train_sample = [1, 5, 5, 2]\n",
    "x_1_train_sample = [1, 3, 4, 1]\n",
    "x_test_sample = [1, 2, 5, 5]\n",
    "x_none_test_sample = [4, 2, 2, 1]\n",
    "for _ in range(20):\n",
    "    low_noise = np.random.normal(0,1,4)\n",
    "    X_train.append(x_0_train_sample + low_noise)\n",
    "    y_train.append([0])\n",
    "    low_noise = np.random.normal(0,1,4)\n",
    "    X_train.append(x_1_train_sample + low_noise)\n",
    "    y_train.append([1])    \n",
    "    \n",
    "    low_noise = np.random.normal(0,1,4)    \n",
    "    X_test.append(x_0_train_sample + low_noise)\n",
    "    y_test.append([0])\n",
    "    low_noise = np.random.normal(0,1,4)    \n",
    "    X_test.append(x_1_train_sample + low_noise)\n",
    "    y_test.append([1])    \n",
    "    low_noise = np.random.normal(0,1,4)\n",
    "    X_test.append(x_test_sample + low_noise)\n",
    "    y_test.append([2])\n",
    "    \n",
    "#     low_noise = np.random.normal(0,1,4)\n",
    "#     X_test.append(x_none_test_sample + low_noise)\n",
    "#     y_test.append([0])    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Armado del dataset\n",
    "\n",
    "x_training_data = np.reshape(X_train,(len(X_train),4,1))\n",
    "x_test_data = np.reshape(X_test,(len(X_test),4,1))\n",
    "y_training_data = y_train\n",
    "y_test_data = y_test\n",
    "\n",
    "# #normalize (x-mean)/std\n",
    "# mean = x_training_data.mean(axis=0)\n",
    "# std = x_training_data.std(axis=0)\n",
    "\n",
    "# x_training_data = x_training_data - mean\n",
    "# x_training_data /= std\n",
    "# x_test_data = x_test_data - mean\n",
    "# x_test_data /= std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_data(X, y, graph, zone_to_remove=None):\n",
    "    dataset = []\n",
    "\n",
    "    for i in range(len(y)):\n",
    "        if y[i] != zone_to_remove:\n",
    "            data = deepcopy(graph)\n",
    "            data['aps'].x = torch.Tensor(X[i])\n",
    "            data['zones'].x = torch.zeros(data['zones'].num_nodes,1)\n",
    "            data['zones'].y = torch.Tensor(y[i])\n",
    "        dataset.append(data)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = build_data(x_training_data, y_training_data, train_graph)\n",
    "test_dataset = build_data(x_test_data, y_test_data, graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "learning_rate = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HeteroGNN_simplified(20, 1, 4).to(device)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(train_dataset, batch_size=len(train_dataset), shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=len(test_dataset), shuffle=False)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=5e-4)\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "train_loss = []\n",
    "train_accuracy = []\n",
    "\n",
    "m = torch.nn.Softmax(dim=1)\n",
    "\n",
    "for epoch in range(150):\n",
    "\n",
    "    # TRAIN\n",
    "    model.train()\n",
    "    train_accuracy_epoch = []\n",
    "    train_loss_epoch = []\n",
    "    optimizer.zero_grad()\n",
    "    for data in train_loader:\n",
    "\n",
    "        data = data.to(device)\n",
    "        out = model(data.x_dict, data.edge_index_dict, data.edge_attr_dict) \n",
    "        out_zones = out[\"zones\"].cpu().reshape(out[\"zones\"].cpu().shape[0]//2,2)\n",
    "\n",
    "        loss_result = loss(out_zones.cpu(), data[\"zones\"].y.cpu().type(torch.long))\n",
    "        loss_result.backward()\n",
    "        train_loss_epoch.append(loss_result.detach().cpu())\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = m(out_zones)\n",
    "        train_accuracy_epoch.append(accuracy_score(data[\"zones\"].y.cpu().reshape(-1).type(torch.long), np.array(torch.argmax(output.cpu(), axis=1))))\n",
    "\n",
    "\n",
    "    train_accuracy.append(np.mean(train_accuracy_epoch))\n",
    "    train_loss.append(np.mean(train_loss_epoch))\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Train Loss {np.mean(train_loss_epoch)}, Train Accuracy {np.mean(train_accuracy_epoch)}\")\n",
    "\n",
    "model.eval()\n",
    "\n",
    "for data in val_loader:\n",
    "    data = data.to(device)\n",
    "    out = model(data.x_dict, data.edge_index_dict, data.edge_attr_dict) \n",
    "    out_zones = out[\"zones\"].cpu().reshape(out[\"zones\"].cpu().shape[0]//2,2)\n",
    "\n",
    "    output = m(out_zones)\n",
    "    print(classification_report(data[\"zones\"].y.cpu().reshape(-1).type(torch.long), np.array(torch.argmax(output.cpu(), axis=1))))\n",
    "    plt.figure()\n",
    "    cf_matrix = confusion_matrix(data[\"zones\"].y.cpu().reshape(-1).type(torch.long), np.array(torch.argmax(output.cpu(), axis=1)), normalize=\"true\")\n",
    "    sns.heatmap(cf_matrix, annot=True, fmt=\".0%\", cmap=\"YlGnBu\", cbar=False)    \n",
    "print(f\"Best Accuracy: Train {np.max(train_accuracy)}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "font = {'size'   : 12}\n",
    "\n",
    "plt.rc('font', **font)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_loader = DataLoader(test_dataset, batch_size=len(test_dataset), shuffle=False)\n",
    "\n",
    "# VALIDATION\n",
    "model.eval()\n",
    "\n",
    "for data in test_loader:\n",
    "    data = data.to(device)\n",
    "    data.x_dict[\"aps\"].requires_grad_()\n",
    "    data.x_dict[\"zones\"].requires_grad_()\n",
    "    out = model(data.x_dict, data.edge_index_dict, data.edge_attr_dict) \n",
    "    out_zones = out[\"zones\"].cpu().reshape(out[\"zones\"].cpu().shape[0]//3,3)\n",
    "\n",
    "    scores = (out_zones.gather(1, data[\"zones\"].y.cpu().view(-1, 1).type(torch.long)).squeeze())\n",
    "    scores.backward(torch.FloatTensor([1.0]*scores.shape[0]))\n",
    "    saliency, _ = torch.max(data.x_dict[\"aps\"].grad.data.abs(), dim=1)\n",
    "    saliency = saliency.reshape((len(test_dataset),4))\n",
    "\n",
    "    output = m(out_zones)\n",
    "    print(classification_report(data[\"zones\"].y.cpu().reshape(-1).type(torch.long), np.array(torch.argmax(output.cpu(), axis=1))))\n",
    "    plt.figure()\n",
    "    cf_matrix = confusion_matrix(data[\"zones\"].y.cpu().reshape(-1).type(torch.long), np.array(torch.argmax(output.cpu(), axis=1)), normalize=\"true\")\n",
    "    sns.heatmap(cf_matrix, annot=True, fmt=\".0%\", cmap=\"YlGnBu\", cbar=False)   \n",
    "\n",
    "plt.savefig(\"Agregado_zona_simple_test.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_loader = DataLoader(test_dataset, batch_size=len(test_dataset), shuffle=False)\n",
    "\n",
    "# VALIDATION\n",
    "model.eval()\n",
    "\n",
    "for data in val_loader:\n",
    "    data = data.to(device)\n",
    "    out = model(data.x_dict, data.edge_index_dict, data.edge_attr_dict) \n",
    "    out_zones = out[\"zones\"].cpu().reshape(out[\"zones\"].cpu().shape[0]//2,2)\n",
    "\n",
    "    output = m(out_zones)\n",
    "    print(classification_report(data[\"zones\"].y.cpu().reshape(-1).type(torch.long), np.array(torch.argmax(output.cpu(), axis=1))))\n",
    "    plt.figure()\n",
    "    cf_matrix = confusion_matrix(data[\"zones\"].y.cpu().reshape(-1).type(torch.long), np.array(torch.argmax(output.cpu(), axis=1)), normalize=\"true\")\n",
    "    sns.heatmap(cf_matrix, annot=True, fmt=\".0%\", cmap=\"YlGnBu\", cbar=False) \n",
    "plt.savefig(\"Agregado_zona_simple_train.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### FCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "clf = MLPClassifier(hidden_layer_sizes=(100,100,100,))\n",
    "clf.classes_ = np.array([0, 1, 2])\n",
    "clf.fit(X_train, y_train)\n",
    "print(clf.classes_)\n",
    "# y_pred_fcnn = clf.predict(X_test)\n",
    "# print(accuracy_score(y_test, y_pred_fcnn))\n",
    "# print(classification_report(y_test, y_pred_fcnn))\n",
    "\n",
    "y_pred_fcnn = clf.predict(X_train)\n",
    "print(accuracy_score(y_train, y_pred_fcnn))\n",
    "print(classification_report(y_train, y_pred_fcnn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feedforward(torch.nn.Module):\n",
    "        def __init__(self, input_size, hidden_size):\n",
    "            super(Feedforward, self).__init__()\n",
    "            self.input_size = input_size\n",
    "            self.hidden_size  = hidden_size\n",
    "            self.fc1 = torch.nn.Linear(self.input_size, self.hidden_size)\n",
    "            self.relu = torch.nn.ReLU()\n",
    "            self.fc2 = torch.nn.Linear(self.hidden_size, 2)\n",
    "            # self.sigmoid = torch.nn.Sigmoid()\n",
    "        def forward(self, x):\n",
    "            hidden = self.fc1(x)\n",
    "            relu = self.relu(hidden)\n",
    "            output = self.fc2(relu)\n",
    "            # output = self.sigmoid(output)\n",
    "            return output\n",
    "\n",
    "        \n",
    "hidden_size = 100\n",
    "model = Feedforward(4, hidden_size)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.001)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.eval()\n",
    "# y_pred = model(torch.tensor(X_test).type(torch.float))\n",
    "# before_train = criterion(y_pred.squeeze(), torch.tensor(y_test).reshape(-1))\n",
    "# print('Test loss before training' , before_train.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "epoch = 300\n",
    "for epoch in range(epoch):\n",
    "    optimizer.zero_grad()\n",
    "    # Forward pass\n",
    "    y_pred = model(torch.tensor(X_train).type(torch.float))\n",
    "    # Compute Loss\n",
    "    loss = criterion(y_pred.squeeze(), torch.tensor(y_train).reshape(-1))\n",
    "   \n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "print('Epoch {}: train loss: {}'.format(epoch, loss.item()))\n",
    "print(accuracy_score(y_train, np.array(torch.argmax(y_pred.cpu(), axis=1))))\n",
    "print(classification_report(y_train, np.array(torch.argmax(y_pred.cpu(), axis=1))))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fc2 = torch.nn.Linear(hidden_size, 3)\n",
    "model.eval()\n",
    "y_pred = model(torch.tensor(X_test).type(torch.float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.array(torch.argmax(y_pred.cpu(), axis=1))\n",
    "print(accuracy_score(y_test, np.array(torch.argmax(y_pred.cpu(), axis=1))))\n",
    "print(classification_report(y_test, np.array(torch.argmax(y_pred.cpu(), axis=1))))    "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "n4qV8TIQYl62",
    "gfqVFAK32Th4",
    "fmC2Dsazdx7i",
    "YHsZyzkAZKlK",
    "gU46OaNwNj7Z",
    "P3QFKooiTRQF",
    "DdJpdEQmWRuO",
    "9Y6Zs61LXiik",
    "MTSumaDohl3K",
    "d2MElifG5G5q",
    "2EjKrCuHim66",
    "iSVeCEtNim7C",
    "1uwrgRzJim7D",
    "kpCBGsGe6J3Z",
    "Tb5Kdlky-4DN",
    "2cJTvncIOZ6I",
    "SwovNzD_THq_",
    "V5fnHhVyXMC4"
   ],
   "name": "tesis_mnav_simple_gnn.ipynb",
   "provenance": [
    {
     "file_id": "1AOWB0JuB1YtTUv8waFAYjdVb27Oi_zVw",
     "timestamp": 1617051239005
    },
    {
     "file_id": "1-veytfLbAuWzFrDTNxXhvsEfn8Th0D2K",
     "timestamp": 1616972473551
    },
    {
     "file_id": "1iQI9YVQL7SxQAvy2A4VAE8jJCv7nBfbI",
     "timestamp": 1615845377065
    }
   ]
  },
  "interpreter": {
   "hash": "feb5c0b0a29f8cb9ce82bbdb934224dc5d3b107b617c193c25692c16f2b91aa2"
  },
  "kernelspec": {
   "display_name": "flezama-tesis",
   "language": "python",
   "name": "flezama-tesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
