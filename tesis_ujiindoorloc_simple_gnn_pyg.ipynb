{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vp0qGc5z4AOk",
    "tags": []
   },
   "source": [
    "# UJIINDOORLOC - Simple GNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oDifkXbM4UUX"
   },
   "source": [
    "Dataset: UJIINDOORLOC\n",
    "\n",
    "Modelo: GNN simple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n4qV8TIQYl62",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Importar Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 27989,
     "status": "ok",
     "timestamp": 1635510361213,
     "user": {
      "displayName": "Facundo Lezama",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjvX7z6H8HKQ9Q4HSRF20NaPjvwtJ3pQh0TGed2XQ=s64",
      "userId": "08104556030349101598"
     },
     "user_tz": 180
    },
    "id": "OjHf3j75Agti"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from itertools import combinations\n",
    "from copy import deepcopy\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "import torch\n",
    "import networkx as nx\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
    "\n",
    "\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn.conv.dna_conv import Linear\n",
    "from torch_geometric.utils import to_networkx, is_undirected, to_undirected\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, ChebConv, SAGEConv, TAGConv, GraphConv\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "from zipfile import ZipFile\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1270,
     "status": "ok",
     "timestamp": 1635454213295,
     "user": {
      "displayName": "Facundo Lezama",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjvX7z6H8HKQ9Q4HSRF20NaPjvwtJ3pQh0TGed2XQ=s64",
      "userId": "08104556030349101598"
     },
     "user_tz": 180
    },
    "id": "jXw5-YfeAgtp"
   },
   "outputs": [],
   "source": [
    "# Descarga de datos\n",
    "!kaggle datasets download -d giantuji/UjiIndoorLoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 383
    },
    "executionInfo": {
     "elapsed": 1105,
     "status": "ok",
     "timestamp": 1635454214395,
     "user": {
      "displayName": "Facundo Lezama",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjvX7z6H8HKQ9Q4HSRF20NaPjvwtJ3pQh0TGed2XQ=s64",
      "userId": "08104556030349101598"
     },
     "user_tz": 180
    },
    "id": "OIhn7QSoAgtq",
    "outputId": "37bf3f47-b29c-4af5-aec4-a23a675b0eef"
   },
   "outputs": [],
   "source": [
    "dataset = 'UjiIndoorLoc.zip'\n",
    "zip_file = ZipFile(dataset)\n",
    "df = pd.read_csv(zip_file.open('TrainingData.csv'))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## BORRAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X = df.iloc[:,:520]\n",
    "df_X.values[df_X.values==100] = -105"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aps_mean = df_X.describe().iloc[1,:]\n",
    "aps_std = df_X.describe().iloc[2,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "font = {'size'   : 15}\n",
    "plt.rc('font', **font)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aps_mean.hist(figsize=[8,6])\n",
    "# aps_std.hist(figsize=[8,6])\n",
    "plt.xlabel(\"RSSI (dBm)\")\n",
    "plt.ylabel(\"Fingerprints\")\n",
    "# plt.show()\n",
    "plt.savefig(\"ujiindoorloc_mean_distribution.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gfqVFAK32Th4",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Preprocesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1635454214397,
     "user": {
      "displayName": "Facundo Lezama",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjvX7z6H8HKQ9Q4HSRF20NaPjvwtJ3pQh0TGed2XQ=s64",
      "userId": "08104556030349101598"
     },
     "user_tz": 180
    },
    "id": "q0BCpSLFAgts"
   },
   "outputs": [],
   "source": [
    "def preprocess_dataset(dataset_path, filter_std, dataset_percentage=None):\n",
    "    \n",
    "    zip_file = ZipFile(dataset_path)\n",
    "    df = pd.read_csv(zip_file.open('TrainingData.csv'))\n",
    "    \n",
    "    df['CLASS'] = df['BUILDINGID'].astype(str) + df['FLOOR'].astype(str)\n",
    "    \n",
    "    df_X = df.iloc[:,:520]\n",
    "    df_y = df['CLASS']\n",
    "\n",
    "    df_X.values[df_X.values==100] = -105\n",
    "\n",
    "\n",
    "    # keep those APs where std > filter_std\n",
    "    ap = (df_X.describe().iloc[2]>filter_std).index\n",
    "    values = (df_X.describe().iloc[2]>filter_std).values\n",
    "    filtered_aps = [ap[i] for i in range(len(values)) if values[i]==True]\n",
    "    df_X = df_X[filtered_aps]\n",
    "        \n",
    "    # take minimum -105 to 0\n",
    "    df_X.iloc[:,:] = 105 + df_X.values\n",
    "    df_X['CLASS'] = df_y.values \n",
    "    \n",
    "    if dataset_percentage:\n",
    "        df_X = df_X.sample(frac=dataset_percentage)       \n",
    "    \n",
    "    # apply ordinal encoder to the classes and split X, y\n",
    "    enc = OrdinalEncoder(dtype=int)\n",
    "    y = enc.fit_transform(df_X['CLASS'].values.reshape(-1,1))\n",
    "    X = df_X.iloc[:,:-1].values    \n",
    "\n",
    "    dfaux = pd.DataFrame(X)\n",
    "\n",
    "    number_aps = len(dfaux.columns)\n",
    "    dfaux[str(number_aps)] = y\n",
    "    subsample = dfaux.sample(frac=1, random_state=99)\n",
    "    y = subsample.iloc[:, -1].values.reshape(-1,1)\n",
    "    X = subsample.iloc[:, :-1].values\n",
    "    \n",
    "    # split 80-20\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "    \n",
    "    print(\"X_train shape: \", X_train.shape)\n",
    "    return X_train, X_test, y_train, y_test, number_aps, len(df_X['CLASS'].value_counts()), filtered_aps, enc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fmC2Dsazdx7i",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Grafo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cF2ZViCq3a_Q"
   },
   "source": [
    "En las siguientes celdas se describe un poco el dataset y se muestran las distribuciones de potencia por AP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1635454218455,
     "user": {
      "displayName": "Facundo Lezama",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjvX7z6H8HKQ9Q4HSRF20NaPjvwtJ3pQh0TGed2XQ=s64",
      "userId": "08104556030349101598"
     },
     "user_tz": 180
    },
    "id": "xHfjTzU5E2eq"
   },
   "outputs": [],
   "source": [
    "def graph_creator(X_G, th=10):\n",
    "    \"\"\"\n",
    "    Dado un dataset y un threshold se arma un grafo basado en las medidas de RRSI\n",
    "    \"\"\"\n",
    "    df_data_train = pd.DataFrame(X_G)\n",
    "    df_G = pd.DataFrame(columns = ['from', 'to', 'weight']) \n",
    "\n",
    "    columns = df_data_train.columns.to_list()\n",
    "    for ap in columns:\n",
    "        # para cada AP me quedo con las instancias donde el RSSI esta en el rango\n",
    "        # (max-th) intentando estimar las instancias mas cercanas al AP\n",
    "        max_val = df_data_train[ap].max()\n",
    "        df_aux_i = df_data_train[df_data_train[ap]  > (max_val - th)]\n",
    "        df_aux_i = df_aux_i.drop(ap, axis=1) \n",
    "        df_aux_i.head()\n",
    "\n",
    "        for k, v in df_aux_i.mean().items():\n",
    "            # armo las aristas con el promedio de RSSI que ven las instancias \n",
    "            # filtradas al resto de los APs\n",
    "            # weight = v\n",
    "            # if df_G.loc[(df_G['from'] == k) & (df_G['to'] == ap)].weight.any():\n",
    "            #     weight = np.mean([float(df_G.loc[(df_G['from'] == k) & (df_G['to'] == ap)].weight), weight])\n",
    "            #     df_G.loc[(df_G['from'] == k) & (df_G['to'] == ap)] = k, ap, weight\n",
    "            df_G = df_G.append({'from':ap, 'to': k, 'weight': v}, ignore_index=True)\n",
    "        \n",
    "\n",
    "    edge_index_first_row = []\n",
    "    edge_index_second_row = []\n",
    "    edge_attr = []\n",
    "    for index, row in df_G.iterrows():\n",
    "        edge_index_first_row.append(columns.index(row['from']))\n",
    "        edge_index_second_row.append(columns.index(row['to']))\n",
    "        edge_attr.append([float(row.weight)])\n",
    "\n",
    "    \n",
    "    edge_index = torch.tensor([edge_index_first_row, edge_index_second_row], dtype=torch.long)\n",
    "    edge_attr = torch.tensor(edge_attr, dtype=torch.float)                           \n",
    "    edge_index, edge_attr = to_undirected(edge_index, edge_attr, reduce=\"mean\")\n",
    "    return edge_index, edge_attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(X, y, graph):\n",
    "    dataset = []\n",
    "    for i in range(len(y)):\n",
    "        data = deepcopy(graph)\n",
    "        data.x = torch.Tensor(X[i])\n",
    "        data.y = torch.Tensor(y[i])\n",
    "        data.train_mask = torch.Tensor([True]*len(y))\n",
    "        data.val_mask = torch.Tensor([True]*len(y))\n",
    "        data.test_mask = torch.Tensor([True]*len(y))                \n",
    "        dataset.append(data)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN_GCNConv(torch.nn.Module):\n",
    "    def __init__(self, number_of_aps, number_of_classes, conv_out_features: list = [16, 20]):\n",
    "        super().__init__()\n",
    "        self.number_of_aps = number_of_aps\n",
    "        self.number_of_classes = number_of_classes\n",
    "        self.conv_out_features = conv_out_features\n",
    "        self.conv1 = GCNConv(1, self.conv_out_features[0], bias=True, normalize=True)\n",
    "        self.conv2 = GCNConv(self.conv_out_features[0], self.conv_out_features[1], bias=True, normalize=True)\n",
    "        self.fc = torch.nn.Linear(self.conv_out_features[-1]*self.number_of_aps, self.number_of_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        # print(\"After Conv1: \", x.shape)\n",
    "\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)        \n",
    "        # print(\"After Conv2: \", x.shape)\n",
    "\n",
    "        # x = torch.flatten(x, 0)\n",
    "        x = torch.reshape(x, (int(x.shape[0]/self.number_of_aps),self.conv_out_features[-1]*self.number_of_aps))\n",
    "        # print(\"After Flatten: \", x.shape)\n",
    "        \n",
    "        x = self.fc(x)\n",
    "        x = F.relu(x)        \n",
    "        # print(\"After FC: \", x.shape)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN_TAGConv(torch.nn.Module):\n",
    "    def __init__(self, number_of_aps, number_of_classes, k: list = [1,1], conv_out_features: list = [16, 20]):\n",
    "        super().__init__()\n",
    "        self.number_of_aps = number_of_aps\n",
    "        self.number_of_classes = number_of_classes        \n",
    "        self.k = k\n",
    "        self.conv_out_features = conv_out_features\n",
    "        self.conv1 = TAGConv(1, self.conv_out_features[0], K=self.k[0], bias=True, normalize=True)\n",
    "        self.conv2 = TAGConv(self.conv_out_features[0], self.conv_out_features[1], K=self.k[1], bias=True, normalize=True)\n",
    "        self.fc = torch.nn.Linear(self.conv_out_features[-1]*self.number_of_aps, self.number_of_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        # print(\"After Conv1: \", x.shape)\n",
    "\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)        \n",
    "        # print(\"After Conv2: \", x.shape)\n",
    "\n",
    "        # x = torch.flatten(x, 0)\n",
    "        x = torch.reshape(x, (int(x.shape[0]/self.number_of_aps),self.conv_out_features[-1]*self.number_of_aps))        \n",
    "        # print(\"After Flatten: \", x.shape)\n",
    "        \n",
    "        x = self.fc(x)\n",
    "        x = F.relu(x)        \n",
    "        # print(\"After FC: \", x.shape)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN_GraphConv(torch.nn.Module):\n",
    "    def __init__(self, number_of_aps, number_of_classes, aggr: str = \"add\", conv_out_features: list = [16, 20]):\n",
    "        super().__init__()\n",
    "        self.number_of_aps = number_of_aps\n",
    "        self.number_of_classes = number_of_classes           \n",
    "        self.aggr = aggr\n",
    "        self.conv_out_features = conv_out_features\n",
    "        self.conv1 = GraphConv(1, self.conv_out_features[0], aggr=self.aggr, bias=True)\n",
    "        self.conv2 = GraphConv(self.conv_out_features[0], self.conv_out_features[1], aggr=self.aggr, bias=True)\n",
    "        self.fc = torch.nn.Linear(self.conv_out_features[-1]*self.number_of_aps, self.number_of_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        # print(\"After Conv1: \", x.shape)\n",
    "\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)        \n",
    "        # print(\"After Conv2: \", x.shape)\n",
    "\n",
    "        # x = torch.flatten(x, 0)\n",
    "        x = torch.reshape(x, (int(x.shape[0]/self.number_of_aps),self.conv_out_features[-1]*self.number_of_aps))        \n",
    "        # print(\"After Flatten: \", x.shape)\n",
    "        \n",
    "        x = self.fc(x)\n",
    "        x = F.relu(x)        \n",
    "        # print(\"After FC: \", x.shape)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YHsZyzkAZKlK",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.edge_attr = (data.edge_attr - data.edge_attr.mean()) / data.edge_attr.std()\n",
    "data.edge_attr = (data.edge_attr - data.edge_attr.min()) / data.edge_attr.max()\n",
    "# data.edge_attr *= 4\n",
    "# data.edge_attr = torch.nn.functional.normalize(data.edge_attr, dim=0)\n",
    "g = to_networkx(data, edge_attrs=[\"edge_attr\"])\n",
    "weights = nx.get_edge_attributes(g,'edge_attr').values()\n",
    "# pos = nx.circular_layout(g)\n",
    "nx.draw(g, width=list(weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 356,
     "status": "ok",
     "timestamp": 1635462144424,
     "user": {
      "displayName": "Facundo Lezama",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjvX7z6H8HKQ9Q4HSRF20NaPjvwtJ3pQh0TGed2XQ=s64",
      "userId": "08104556030349101598"
     },
     "user_tz": 180
    },
    "id": "0MsSCtZkUDbj"
   },
   "outputs": [],
   "source": [
    "# Split de los datos y armado del objeto Data con el grafo\n",
    "\n",
    "X_train, X_test, y_train, y_test, num_aps, num_classes, filtered_aps, enc_train = preprocess_dataset(dataset, filter_std=3, dataset_percentage=0.4)\n",
    "edge_index, edge_attr = graph_creator(X_train[:,:-1], th=10)\n",
    "data = Data(edge_index=edge_index, edge_attr=edge_attr, num_nodes=num_aps)\n",
    "print(f\"Undirected: {data.is_undirected()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Armado del dataset\n",
    "\n",
    "x_training_data = np.reshape(X_train,(X_train.shape[0],num_aps,1))\n",
    "x_test_data = np.reshape(X_test,(X_test.shape[0],num_aps,1))\n",
    "y_training_data = y_train\n",
    "y_test_data = y_test\n",
    "\n",
    "#normalize (x-mean)/std\n",
    "mean = x_training_data.mean(axis=0)\n",
    "std = x_training_data.std(axis=0)\n",
    "\n",
    "x_training_data = x_training_data - mean\n",
    "x_training_data /= std\n",
    "x_test_data = x_test_data - mean\n",
    "x_test_data /= std\n",
    "\n",
    "train_dataset = build_dataset(x_training_data, y_training_data, data)\n",
    "test_dataset = build_dataset(x_test_data, y_test_data, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### GCNConv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GNN_GCNConv(num_aps, num_classes, conv_out_features=[20,20]).to(device)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=5e-4)\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = []\n",
    "train_accuracy = []\n",
    "test_loss = []\n",
    "test_accuracy = []\n",
    "\n",
    "m = torch.nn.Softmax(dim=1)\n",
    "\n",
    "for epoch in range(100):\n",
    "    print(f\"Epoch: {epoch}\")\n",
    "    \n",
    "    # TRAIN\n",
    "    model.train()\n",
    "    train_accuracy_epoch = []\n",
    "    train_loss_epoch = []\n",
    "    for data in train_loader:\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        # print(data.x.shape)\n",
    "        # print(data.y.shape)\n",
    "        \n",
    "        \n",
    "        out = model(data.to(device))\n",
    "        # print(out)\n",
    "        # out_softmax = np.array(torch.argmax(out, dim=0)).item()\n",
    "        # out_softmax = torch.tensor([out_softmax])\n",
    "        loss_result = loss(out, data.y.type(torch.long))        \n",
    "        loss_result.backward()\n",
    "        \n",
    "        train_loss_epoch.append(loss_result.detach().cpu())\n",
    "        output = m(out)\n",
    "        train_accuracy_epoch.append(accuracy_score(data.y.cpu().reshape(-1).type(torch.long), np.array(torch.argmax(output.cpu(), axis=1))))\n",
    "\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "    if scheduler.get_last_lr()[0] > 0.0005:\n",
    "        scheduler.step()\n",
    "\n",
    "    train_accuracy.append(np.mean(train_accuracy_epoch))\n",
    "    train_loss.append(np.mean(train_loss_epoch))\n",
    "\n",
    "    # VALIDATION\n",
    "    model.eval()\n",
    "    test_accuracy_epoch = []\n",
    "    test_loss_epoch = []\n",
    "    for data in test_loader:\n",
    "        out = model(data.to(device))\n",
    "        loss_result = loss(out, data.y.type(torch.long))        \n",
    "        \n",
    "        test_loss_epoch.append(loss_result.detach().cpu())\n",
    "        output = m(out)\n",
    "        test_accuracy_epoch.append(accuracy_score(data.y.cpu().reshape(-1).type(torch.long), np.array(torch.argmax(output.cpu(), axis=1))))\n",
    "\n",
    "    test_accuracy.append(np.mean(test_accuracy_epoch))\n",
    "    test_loss.append(np.mean(test_loss_epoch))\n",
    "\n",
    "\n",
    "print(f\"Last LR: {scheduler.get_last_lr()}\")\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(train_loss, label=\"Train loss\")\n",
    "plt.plot(test_loss, label=\"Validation loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(train_accuracy, label=\"Train accuracy\")\n",
    "plt.plot(test_accuracy, label=\"Validation accuracy\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = torch.nn.Softmax(dim=1)\n",
    "output = m(out)\n",
    "accuracy = accuracy_score(data.y.cpu().reshape(-1).type(torch.long), np.array(torch.argmax(output.cpu(), axis=1)))\n",
    "\n",
    "print(accuracy)\n",
    "print(classification_report(data.y.cpu().reshape(-1).type(torch.long), np.array(torch.argmax(output.cpu(), axis=1))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"GCNConv_best_model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### TAGConv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GNN_TAGConv(num_aps, num_classes, k=[2,2], conv_out_features=[20,20]).to(device)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=5e-4)\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = []\n",
    "train_accuracy = []\n",
    "test_loss = []\n",
    "test_accuracy = []\n",
    "\n",
    "m = torch.nn.Softmax(dim=1)\n",
    "\n",
    "for epoch in range(100):\n",
    "    print(f\"Epoch: {epoch}\")\n",
    "    \n",
    "    # TRAIN\n",
    "    model.train()\n",
    "    train_accuracy_epoch = []\n",
    "    train_loss_epoch = []\n",
    "    for data in train_loader:\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        # print(data.x.shape)\n",
    "        # print(data.y.shape)\n",
    "        \n",
    "        \n",
    "        out = model(data.to(device))\n",
    "        # print(out)\n",
    "        # out_softmax = np.array(torch.argmax(out, dim=0)).item()\n",
    "        # out_softmax = torch.tensor([out_softmax])\n",
    "        loss_result = loss(out, data.y.type(torch.long))        \n",
    "        loss_result.backward()\n",
    "        \n",
    "        train_loss_epoch.append(loss_result.detach().cpu())\n",
    "        output = m(out)\n",
    "        train_accuracy_epoch.append(accuracy_score(data.y.cpu().reshape(-1).type(torch.long), np.array(torch.argmax(output.cpu(), axis=1))))\n",
    "\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "    # if scheduler.get_last_lr()[0] > 0.0005:\n",
    "    #     scheduler.step()\n",
    "    if (epoch+1)%10 == 0:\n",
    "        scheduler.step()\n",
    "        \n",
    "    train_accuracy.append(np.mean(train_accuracy_epoch))\n",
    "    train_loss.append(np.mean(train_loss_epoch))\n",
    "\n",
    "    # VALIDATION\n",
    "    model.eval()\n",
    "    test_accuracy_epoch = []\n",
    "    test_loss_epoch = []\n",
    "    for data in test_loader:\n",
    "        out = model(data.to(device))\n",
    "        loss_result = loss(out, data.y.type(torch.long))        \n",
    "        \n",
    "        test_loss_epoch.append(loss_result.detach().cpu())\n",
    "        output = m(out)\n",
    "        test_accuracy_epoch.append(accuracy_score(data.y.cpu().reshape(-1).type(torch.long), np.array(torch.argmax(output.cpu(), axis=1))))\n",
    "\n",
    "    test_accuracy.append(np.mean(test_accuracy_epoch))\n",
    "    test_loss.append(np.mean(test_loss_epoch))\n",
    "\n",
    "\n",
    "print(f\"Last LR: {scheduler.get_last_lr()}\")\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(train_loss, label=\"Train loss\")\n",
    "plt.plot(test_loss, label=\"Validation loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(train_accuracy, label=\"Train accuracy\")\n",
    "plt.plot(test_accuracy, label=\"Validation accuracy\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = torch.nn.Softmax(dim=1)\n",
    "output = m(out)\n",
    "accuracy = accuracy_score(data.y.cpu().reshape(-1).type(torch.long), np.array(torch.argmax(output.cpu(), axis=1)))\n",
    "\n",
    "print(accuracy)\n",
    "print(classification_report(data.y.cpu().reshape(-1).type(torch.long), np.array(torch.argmax(output.cpu(), axis=1))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), \"TAGConv_best_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"UJI_Simple_porc0.4_12_best_model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### GraphConv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GNN_GraphConv(num_aps, num_classes, aggr=\"mean\", conv_out_features=[20,20]).to(device)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=5e-4)\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = []\n",
    "train_accuracy = []\n",
    "test_loss = []\n",
    "test_accuracy = []\n",
    "\n",
    "m = torch.nn.Softmax(dim=1)\n",
    "\n",
    "for epoch in range(100):\n",
    "    print(f\"Epoch: {epoch}\")\n",
    "    \n",
    "    # TRAIN\n",
    "    model.train()\n",
    "    train_accuracy_epoch = []\n",
    "    train_loss_epoch = []\n",
    "    for data in train_loader:\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        # print(data.x.shape)\n",
    "        # print(data.y.shape)\n",
    "        \n",
    "        \n",
    "        out = model(data.to(device))\n",
    "        # print(out)\n",
    "        # out_softmax = np.array(torch.argmax(out, dim=0)).item()\n",
    "        # out_softmax = torch.tensor([out_softmax])\n",
    "        loss_result = loss(out, data.y.type(torch.long))        \n",
    "        loss_result.backward()\n",
    "        \n",
    "        train_loss_epoch.append(loss_result.detach().cpu())\n",
    "        output = m(out)\n",
    "        train_accuracy_epoch.append(accuracy_score(data.y.cpu().reshape(-1).type(torch.long), np.array(torch.argmax(output.cpu(), axis=1))))\n",
    "\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "    if scheduler.get_last_lr()[0] > 0.0005:\n",
    "        scheduler.step()\n",
    "\n",
    "    train_accuracy.append(np.mean(train_accuracy_epoch))\n",
    "    train_loss.append(np.mean(train_loss_epoch))\n",
    "\n",
    "    # VALIDATION\n",
    "    model.eval()\n",
    "    test_accuracy_epoch = []\n",
    "    test_loss_epoch = []\n",
    "    for data in test_loader:\n",
    "        out = model(data.to(device))\n",
    "        loss_result = loss(out, data.y.type(torch.long))        \n",
    "        \n",
    "        test_loss_epoch.append(loss_result.detach().cpu())\n",
    "        output = m(out)\n",
    "        test_accuracy_epoch.append(accuracy_score(data.y.cpu().reshape(-1).type(torch.long), np.array(torch.argmax(output.cpu(), axis=1))))\n",
    "\n",
    "    test_accuracy.append(np.mean(test_accuracy_epoch))\n",
    "    test_loss.append(np.mean(test_loss_epoch))\n",
    "\n",
    "\n",
    "print(f\"Last LR: {scheduler.get_last_lr()}\")\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(train_loss, label=\"Train loss\")\n",
    "plt.plot(test_loss, label=\"Validation loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(train_accuracy, label=\"Train accuracy\")\n",
    "plt.plot(test_accuracy, label=\"Validation accuracy\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = torch.nn.Softmax(dim=1)\n",
    "output = m(out)\n",
    "accuracy = accuracy_score(data.y.cpu().reshape(-1).type(torch.long), np.array(torch.argmax(output.cpu(), axis=1)))\n",
    "\n",
    "print(accuracy)\n",
    "print(classification_report(data.y.cpu().reshape(-1).type(torch.long), np.array(torch.argmax(output.cpu(), axis=1))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"GraphConv_best_model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'UjiIndoorLoc.zip'\n",
    "zip_file = ZipFile(dataset)\n",
    "df_test = pd.read_csv(zip_file.open('ValidationData.csv'))\n",
    "df.head()\n",
    "\n",
    "df_test['CLASS'] = df_test['BUILDINGID'].astype(str) + df_test['FLOOR'].astype(str)\n",
    "\n",
    "df_X_test = df_test[filtered_aps]\n",
    "df_y_test = df_test['CLASS']\n",
    "print(df_X_test.shape)\n",
    "\n",
    "df_X_test.values[df_X_test.values==100] = -105\n",
    "df_X_test.iloc[:,:] = 105 + df_X_test.values\n",
    "df_X_test['CLASS'] = df_y_test.values \n",
    "\n",
    "df_X_test.describe()\n",
    "\n",
    "\n",
    "y_test = enc_train.transform(df_X_test['CLASS'].values.reshape(-1,1))\n",
    "X_test = df_X_test.iloc[:,:-1].values\n",
    "\n",
    "\n",
    "# Armado del dataset\n",
    "\n",
    "x_test_data = np.reshape(X_test,(X_test.shape[0],num_aps,1))\n",
    "y_test_data = y_test\n",
    "\n",
    "#normalize (x-mean)/std\n",
    "\n",
    "x_test_data = x_test_data - mean\n",
    "x_test_data /= std\n",
    "\n",
    "test_dataset = build_dataset(x_test_data, y_test_data, data)\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = GNN_GraphConv(num_aps, num_classes, aggr=\"mean\", conv_out_features=[20,20]).to(device)\n",
    "# model.load_state_dict(torch.load(\"GraphConv_best_model.pth\"))\n",
    "\n",
    "model = GNN_TAGConv(num_aps, num_classes, k=[2,2], conv_out_features=[20,20])\n",
    "model.load_state_dict(torch.load(\"checkpoints/TAGConv_best_model.pth\"))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = []\n",
    "train_accuracy = []\n",
    "test_loss = []\n",
    "test_accuracy = []\n",
    "\n",
    "m = torch.nn.Softmax(dim=1)\n",
    "\n",
    "# TEST\n",
    "model.eval()\n",
    "for data in test_loader:\n",
    "    out = model(data.to(device))\n",
    "    \n",
    "    output = m(out)\n",
    "    test_accuracy.append(accuracy_score(data.y.cpu().reshape(-1).type(torch.long), np.array(torch.argmax(output.cpu(), axis=1))))\n",
    "\n",
    "total_test_accuracy = np.mean(test_accuracy)\n",
    "\n",
    "print(f\"TEST ACCURACY: {total_test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = torch.nn.Softmax(dim=1)\n",
    "output = m(out)\n",
    "accuracy = accuracy_score(data.y.cpu().reshape(-1).type(torch.long), np.array(torch.argmax(output.cpu(), axis=1)))\n",
    "\n",
    "print(accuracy)\n",
    "print(classification_report(data.y.cpu().reshape(-1).type(torch.long), np.array(torch.argmax(output.cpu(), axis=1))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruebas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from zipfile import ZipFile\n",
    "import torch\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn.conv.dna_conv import Linear\n",
    "from torch_geometric.utils import to_networkx, is_undirected, to_undirected\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, ChebConv, SAGEConv, TAGConv, GraphConv\n",
    "from torch_geometric.loader import DataLoader\n",
    "from copy import deepcopy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## KNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "neigh = KNeighborsClassifier(n_neighbors=15)\n",
    "param = {'n_neighbors': [5, 10, 15]}\n",
    "\n",
    "clf_knn= GridSearchCV(neigh, param)\n",
    "clf_knn.fit(X_train, y_train.ravel())\n",
    "\n",
    "\n",
    "print(clf_knn.best_params_)\n",
    "print(clf_knn.best_score_)\n",
    "\n",
    "K = clf_knn.best_params_['n_neighbors']\n",
    "neigh = KNeighborsClassifier(n_neighbors=K)\n",
    "neigh.fit(X_train, y_train.ravel())\n",
    "y_pred_knn = neigh.predict(X_test)\n",
    "\n",
    "print(accuracy_score(y_test, y_pred_knn))\n",
    "print(classification_report(y_test, y_pred_knn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "neigh = KNeighborsClassifier(n_neighbors=5)\n",
    "neigh.fit(X_train, y_train.ravel())\n",
    "y_pred_knn = neigh.predict(X_test)\n",
    "\n",
    "print(accuracy_score(y_test, y_pred_knn))\n",
    "print(classification_report(y_test, y_pred_knn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Análisis variando cantidad de muestras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "print_every = 5\n",
    "\n",
    "porcentajes = [0.9, 1] # [0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
    "accuracy = {\"0.3\":[], \"0.4\":[], \"0.5\":[], \"0.6\":[], \"0.7\":[], \"0.8\":[], \"0.9\":[], \"1\":[]}\n",
    "\n",
    "for porc in porcentajes:\n",
    "    print('Porcentaje de datos: ', porc)\n",
    "    \n",
    "    for i in range(4):    \n",
    "\n",
    "        # Split de los datos y armado del objeto Data con el grafo\n",
    "\n",
    "        X_train, X_test, y_train, y_test, num_aps, num_classes, filtered_aps, enc_train = preprocess_dataset(dataset, filter_std=3, dataset_percentage=porc)\n",
    "        edge_index, edge_attr = graph_creator(X_train[:,:-1], th=10)\n",
    "        data = Data(edge_index=edge_index, edge_attr=edge_attr, num_nodes=num_aps)\n",
    "\n",
    "        # Armado del dataset\n",
    "\n",
    "        x_training_data = np.reshape(X_train,(X_train.shape[0],num_aps,1))\n",
    "        x_test_data = np.reshape(X_test,(X_test.shape[0],num_aps,1))\n",
    "        y_training_data = y_train\n",
    "        y_test_data = y_test\n",
    "\n",
    "        #normalize (x-mean)/std\n",
    "        mean = x_training_data.mean(axis=0)\n",
    "        std = x_training_data.std(axis=0)\n",
    "\n",
    "        x_training_data = x_training_data - mean\n",
    "        x_training_data /= std\n",
    "        x_test_data = x_test_data - mean\n",
    "        x_test_data /= std\n",
    "\n",
    "        train_dataset = build_dataset(x_training_data, y_training_data, data)\n",
    "        test_dataset = build_dataset(x_test_data, y_test_data, data)\n",
    "\n",
    "        model = GNN_TAGConv(num_aps, num_classes, k=[2,2], conv_out_features=[20,20]).to(device)\n",
    "\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=5e-4)\n",
    "        loss = torch.nn.CrossEntropyLoss()\n",
    "        scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.8)\n",
    "\n",
    "        \n",
    "        train_loss = []\n",
    "        train_accuracy = []\n",
    "        test_loss = []\n",
    "        test_accuracy = []\n",
    "        best_test_accuracy = 0\n",
    "        \n",
    "        m = torch.nn.Softmax(dim=1)\n",
    "\n",
    "        for epoch in range(100):\n",
    "            # print(f\"Epoch: {epoch}\")\n",
    "\n",
    "            # TRAIN\n",
    "            model.train()\n",
    "            train_accuracy_epoch = []\n",
    "            train_loss_epoch = []\n",
    "            for data in train_loader:\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                # print(data.x.shape)\n",
    "                # print(data.y.shape)\n",
    "\n",
    "\n",
    "                out = model(data.to(device))\n",
    "                # print(out)\n",
    "                # out_softmax = np.array(torch.argmax(out, dim=0)).item()\n",
    "                # out_softmax = torch.tensor([out_softmax])\n",
    "                loss_result = loss(out, data.y.type(torch.long))        \n",
    "                loss_result.backward()\n",
    "\n",
    "                train_loss_epoch.append(loss_result.detach().cpu())\n",
    "                output = m(out)\n",
    "                train_accuracy_epoch.append(accuracy_score(data.y.cpu().reshape(-1).type(torch.long), np.array(torch.argmax(output.cpu(), axis=1))))\n",
    "\n",
    "\n",
    "                optimizer.step()\n",
    "\n",
    "            # if scheduler.get_last_lr()[0] > 0.0005:\n",
    "            #     scheduler.step()\n",
    "            if (epoch+1)%10 == 0:\n",
    "                scheduler.step()\n",
    "\n",
    "            train_accuracy.append(np.mean(train_accuracy_epoch))\n",
    "            train_loss.append(np.mean(train_loss_epoch))\n",
    "\n",
    "            # VALIDATION\n",
    "            model.eval()\n",
    "            test_accuracy_epoch = []\n",
    "            test_loss_epoch = []\n",
    "            for data in test_loader:\n",
    "                out = model(data.to(device))\n",
    "                loss_result = loss(out, data.y.type(torch.long))        \n",
    "\n",
    "                test_loss_epoch.append(loss_result.detach().cpu())\n",
    "                output = m(out)\n",
    "                test_accuracy_epoch.append(accuracy_score(data.y.cpu().reshape(-1).type(torch.long), np.array(torch.argmax(output.cpu(), axis=1))))\n",
    "\n",
    "            test_accuracy.append(np.mean(test_accuracy_epoch))\n",
    "            if test_accuracy[-1] > best_test_accuracy:\n",
    "                best_test_accuracy = test_accuracy[-1]\n",
    "                torch.save(model.state_dict(), f\"UJI_Simple_porc{porc}_{i}_best_model.pth\")\n",
    "\n",
    "            test_loss.append(np.mean(test_loss_epoch))\n",
    "\n",
    "        print(f\"Best Accuracy: Train {np.max(train_accuracy)}, Val {np.max(test_accuracy)}\")\n",
    "        accuracy[str(porc)].append(np.max(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'UjiIndoorLoc.zip'\n",
    "zip_file = ZipFile(dataset)\n",
    "df_test = pd.read_csv(zip_file.open('ValidationData.csv'))\n",
    "df.head()\n",
    "\n",
    "df_test['CLASS'] = df_test['BUILDINGID'].astype(str) + df_test['FLOOR'].astype(str)\n",
    "\n",
    "df_X_test = df_test[filtered_aps]\n",
    "df_y_test = df_test['CLASS']\n",
    "print(df_X_test.shape)\n",
    "\n",
    "df_X_test.values[df_X_test.values==100] = -105\n",
    "df_X_test.iloc[:,:] = 105 + df_X_test.values\n",
    "df_X_test['CLASS'] = df_y_test.values \n",
    "\n",
    "df_X_test.describe()\n",
    "\n",
    "\n",
    "y_test = enc_train.transform(df_X_test['CLASS'].values.reshape(-1,1))\n",
    "X_test = df_X_test.iloc[:,:-1].values\n",
    "\n",
    "\n",
    "# Armado del dataset\n",
    "\n",
    "x_test_data = np.reshape(X_test,(X_test.shape[0],num_aps,1))\n",
    "y_test_data = y_test\n",
    "\n",
    "#normalize (x-mean)/std\n",
    "\n",
    "x_test_data = x_test_data - mean\n",
    "x_test_data /= std\n",
    "\n",
    "test_dataset = build_dataset(x_test_data, y_test_data, data)\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "porcentajes = [0.4] #, 0.6, 0.7, 0.8, 0.9, 1]\n",
    "accuracy = {\"0.3\":[], \"0.4\":[], \"0.5\":[], \"0.6\":[], \"0.7\":[], \"0.8\":[], \"0.9\":[], \"1\":[]}\n",
    "\n",
    "for porc in porcentajes:\n",
    "    for i in range(13):\n",
    "        model = GNN_TAGConv(num_aps, num_classes, k=[2,2], conv_out_features=[20,20])\n",
    "        model.load_state_dict(torch.load(f\"checkpoints/UJI_Simple_porc{porc}_{i}_best_model.pth\"))\n",
    "        model.to(device)\n",
    "        \n",
    "        test_loss = []\n",
    "        test_accuracy = []\n",
    "\n",
    "        m = torch.nn.Softmax(dim=1)\n",
    "\n",
    "        # TEST\n",
    "        model.eval()\n",
    "        test_accuracy_epoch = []\n",
    "        test_loss_epoch = []\n",
    "        for data in test_loader:\n",
    "            out = model(data.to(device))\n",
    "\n",
    "            output = m(out)\n",
    "            test_accuracy_epoch.append(accuracy_score(data.y.cpu().reshape(-1).type(torch.long), np.array(torch.argmax(output.cpu(), axis=1))))\n",
    "\n",
    "        test_accuracy.append(np.mean(test_accuracy_epoch))\n",
    "        accuracy[str(porc)].append(np.mean(test_accuracy)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "porcentajes = [0.6, 0.7, 0.8, 0.9, 1] # [0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
    "# accuracy = {\"0.3\":[], \"0.4\":[], \"0.5\":[], \"0.6\":[], \"0.7\":[], \"0.8\":[], \"0.9\":[], \"1\":[]}\n",
    "\n",
    "for porc in porcentajes:\n",
    "    for i in range(5):\n",
    "        model = GNN_TAGConv(num_aps, num_classes, k=[2,2], conv_out_features=[20,20])\n",
    "        model.load_state_dict(torch.load(f\"UJI_Simple_porc{porc}_{i}_best_model.pth\"))\n",
    "        model.to(device)\n",
    "        \n",
    "        test_loss = []\n",
    "        test_accuracy = []\n",
    "\n",
    "        m = torch.nn.Softmax(dim=1)\n",
    "\n",
    "        # TEST\n",
    "        model.eval()\n",
    "        test_accuracy_epoch = []\n",
    "        test_loss_epoch = []\n",
    "        for data in test_loader:\n",
    "            out = model(data.to(device))\n",
    "\n",
    "            output = m(out)\n",
    "            test_accuracy_epoch.append(accuracy_score(data.y.cpu().reshape(-1).type(torch.long), np.array(torch.argmax(output.cpu(), axis=1))))\n",
    "\n",
    "        test_accuracy.append(np.mean(test_accuracy_epoch))\n",
    "        accuracy[str(porc)].append(np.mean(test_accuracy)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_aps"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "n4qV8TIQYl62",
    "gfqVFAK32Th4",
    "fmC2Dsazdx7i",
    "YHsZyzkAZKlK",
    "gU46OaNwNj7Z",
    "P3QFKooiTRQF",
    "DdJpdEQmWRuO",
    "9Y6Zs61LXiik",
    "MTSumaDohl3K",
    "d2MElifG5G5q",
    "2EjKrCuHim66",
    "iSVeCEtNim7C",
    "1uwrgRzJim7D",
    "kpCBGsGe6J3Z",
    "Tb5Kdlky-4DN",
    "2cJTvncIOZ6I",
    "SwovNzD_THq_",
    "V5fnHhVyXMC4"
   ],
   "name": "tesis_mnav_simple_gnn.ipynb",
   "provenance": [
    {
     "file_id": "1AOWB0JuB1YtTUv8waFAYjdVb27Oi_zVw",
     "timestamp": 1617051239005
    },
    {
     "file_id": "1-veytfLbAuWzFrDTNxXhvsEfn8Th0D2K",
     "timestamp": 1616972473551
    },
    {
     "file_id": "1iQI9YVQL7SxQAvy2A4VAE8jJCv7nBfbI",
     "timestamp": 1615845377065
    }
   ]
  },
  "interpreter": {
   "hash": "feb5c0b0a29f8cb9ce82bbdb934224dc5d3b107b617c193c25692c16f2b91aa2"
  },
  "kernelspec": {
   "display_name": "pyg-test",
   "language": "python",
   "name": "pyg-test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
