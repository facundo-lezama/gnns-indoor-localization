{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vp0qGc5z4AOk"
   },
   "source": [
    "# MNAV - Simple GNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oDifkXbM4UUX"
   },
   "source": [
    "Dataset: MNAV\n",
    "\n",
    "Modelo: GNN simple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n4qV8TIQYl62",
    "tags": []
   },
   "source": [
    "## Importar Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 27989,
     "status": "ok",
     "timestamp": 1635510361213,
     "user": {
      "displayName": "Facundo Lezama",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjvX7z6H8HKQ9Q4HSRF20NaPjvwtJ3pQh0TGed2XQ=s64",
      "userId": "08104556030349101598"
     },
     "user_tz": 180
    },
    "id": "OjHf3j75Agti"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from itertools import combinations\n",
    "from copy import deepcopy\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "import torch\n",
    "import networkx as nx\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn.conv.dna_conv import Linear\n",
    "from torch_geometric.utils import to_networkx, is_undirected, to_undirected\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, ChebConv, SAGEConv, TAGConv, GraphConv\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cMFznhF9wDxB"
   },
   "source": [
    "El dataset se encuentra disponible en https://github.com/ffedee7/posifi_mnav/tree/master/data_analysis. El dataset disponible está anonimizado pero se puede deshacer con el código de ese repositorio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1270,
     "status": "ok",
     "timestamp": 1635454213295,
     "user": {
      "displayName": "Facundo Lezama",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjvX7z6H8HKQ9Q4HSRF20NaPjvwtJ3pQh0TGed2XQ=s64",
      "userId": "08104556030349101598"
     },
     "user_tz": 180
    },
    "id": "jXw5-YfeAgtp"
   },
   "outputs": [],
   "source": [
    "dataset = 'datos2.csv'\n",
    "df = pd.read_csv(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 383
    },
    "executionInfo": {
     "elapsed": 1105,
     "status": "ok",
     "timestamp": 1635454214395,
     "user": {
      "displayName": "Facundo Lezama",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjvX7z6H8HKQ9Q4HSRF20NaPjvwtJ3pQh0TGed2XQ=s64",
      "userId": "08104556030349101598"
     },
     "user_tz": 180
    },
    "id": "OIhn7QSoAgtq",
    "outputId": "37bf3f47-b29c-4af5-aec4-a23a675b0eef"
   },
   "outputs": [],
   "source": [
    "print(df.shape)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1635454214396,
     "user": {
      "displayName": "Facundo Lezama",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjvX7z6H8HKQ9Q4HSRF20NaPjvwtJ3pQh0TGed2XQ=s64",
      "userId": "08104556030349101598"
     },
     "user_tz": 180
    },
    "id": "hDAHd9mXpwec",
    "outputId": "dbf5f05a-eb40-47e6-c075-1e7fd39f104f"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gfqVFAK32Th4",
    "tags": []
   },
   "source": [
    "## Preprocesamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CL6O8p3ZERrS"
   },
   "source": [
    "Se definen los APs que se quieren usar tanto para la construcción del grafo como para el modelo.\n",
    "\n",
    "En este caso usamos solamente los APs que se encuentran dentro del MNAV, es decir que descartamos los APs que aparecen en las medidas pero que no son los que se instalaron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1635454214396,
     "user": {
      "displayName": "Facundo Lezama",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjvX7z6H8HKQ9Q4HSRF20NaPjvwtJ3pQh0TGed2XQ=s64",
      "userId": "08104556030349101598"
     },
     "user_tz": 180
    },
    "id": "ynWQ8U8z50to"
   },
   "outputs": [],
   "source": [
    "APs_MAC_2_4 = ['wifi-dc:a5:f4:43:85:c0',\n",
    "'wifi-dc:a5:f4:43:27:e0',\n",
    "'wifi-f8:4f:57:ab:da:00',\n",
    "'wifi-5c:a4:8a:4c:05:c0',\n",
    "'wifi-1c:1d:86:ce:ef:b0',\n",
    "'wifi-dc:a5:f4:43:79:20',\n",
    "'wifi-c0:7b:bc:36:9e:10',\n",
    "'wifi-1c:1d:86:9f:99:20',\n",
    "'wifi-c0:7b:bc:36:af:40',\n",
    "'wifi-c0:7b:bc:36:af:80',\n",
    "'wifi-1c:1d:86:b6:ac:80',\n",
    "'wifi-dc:a5:f4:43:72:e0',\n",
    "'wifi-f8:4f:57:ab:d8:60',\n",
    "'wifi-dc:a5:f4:43:72:90',\n",
    "'wifi-f8:4f:57:ab:ce:20']\n",
    "\n",
    "APs_MAC_5 = ['wifi-dc:a5:f4:45:85:b0',\n",
    "'wifi-dc:a5:f4:45:27:e0',\n",
    "'wifi-f8:4f:57:ad:d9:60',\n",
    "'wifi-5c:a4:8a:4e:05:30',\n",
    "'wifi-1c:1d:86:d0:ef:00',\n",
    "'wifi-dc:a5:f4:45:79:10',\n",
    "'wifi-c0:7b:bc:38:9e:00',\n",
    "'wifi-1c:1d:86:a1:99:00',\n",
    "'wifi-c0:7b:bc:38:af:30',\n",
    "'wifi-c0:7b:bc:38:af:70',\n",
    "'wifi-1c:1d:86:b8:ac:80',\n",
    "'wifi-dc:a5:f4:45:72:d0',\n",
    "'wifi-f8:4f:57:ad:d7:c0',\n",
    "'wifi-dc:a5:f4:45:72:80',\n",
    "'wifi-f8:4f:57:ad:cd:80']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1635454214397,
     "user": {
      "displayName": "Facundo Lezama",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjvX7z6H8HKQ9Q4HSRF20NaPjvwtJ3pQh0TGed2XQ=s64",
      "userId": "08104556030349101598"
     },
     "user_tz": 180
    },
    "id": "q0BCpSLFAgts"
   },
   "outputs": [],
   "source": [
    "def preprocess_dataset(dataset_path, dataset_percentage=None):\n",
    "    df = pd.read_csv(dataset_path)\n",
    "    # paso los NaN a 0\n",
    "    df = df.fillna(0) \n",
    "\n",
    "    # sumo 100 a los valores de RSSI y ahora 0 es el minimo\n",
    "    df.iloc[:,1:] = 100 + df.iloc[:,1:]\n",
    "    values = df.iloc[:,1:]\n",
    "\n",
    "    # las medidas originales en 0 las asumo como que estaban muy lejos\n",
    "    # entonces las dejo en 0 que es el nuevo valor minimo\n",
    "    values[values==100] = 0 \n",
    "    df.iloc[:,1:] = values\n",
    "\n",
    "    # armo dos datsets: uno con las medidas solamente de la frecuencia\n",
    "    # 2.4GHz y otro con las frecuencias 2.4GHz y 5GHz\n",
    "    data_2_4 = df[['location'] + APs_MAC_2_4] # REVISAR PORQUE CREO QUE NO LO VUELVO A USAR\n",
    "    data_2_4_5 = df[['location'] + APs_MAC_2_4 + APs_MAC_5]\n",
    "\n",
    "    if dataset_percentage:\n",
    "        data_2_4_5 = data_2_4_5.sample(frac=dataset_percentage)     \n",
    "    \n",
    "    # paso las zonas por un ordinal encoder\n",
    "    enc = OrdinalEncoder(dtype=int)\n",
    "    y = enc.fit_transform(data_2_4_5['location'].values.reshape(-1,1))\n",
    "    X = data_2_4_5.iloc[:,1:].values\n",
    "\n",
    "    # print(enc.categories_)\n",
    "\n",
    "    # separo el dataset en train y test 80-20\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n",
    "    print(\"X_train shape: \", X_train.shape)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fmC2Dsazdx7i",
    "tags": []
   },
   "source": [
    "## Grafo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cF2ZViCq3a_Q"
   },
   "source": [
    "En las siguientes celdas se describe un poco el dataset y se muestran las distribuciones de potencia por AP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1635454218455,
     "user": {
      "displayName": "Facundo Lezama",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjvX7z6H8HKQ9Q4HSRF20NaPjvwtJ3pQh0TGed2XQ=s64",
      "userId": "08104556030349101598"
     },
     "user_tz": 180
    },
    "id": "xHfjTzU5E2eq"
   },
   "outputs": [],
   "source": [
    "def graph_creator(X_G, th=10, cols=None):\n",
    "    \"\"\"\n",
    "    Dado un dataset y un threshold se arma un grafo basado en las medidas de RRSI\n",
    "    \"\"\"\n",
    "\n",
    "    columns = cols if cols else ['AP1', 'AP2', 'AP3', 'AP4', 'AP5', 'AP6', 'AP7', 'AP8', 'AP9', 'AP10', 'AP11', 'AP12', 'AP13', 'AP14', 'AP15']\n",
    "    df_data_train = pd.DataFrame(X_G, columns=columns)\n",
    "    df_G = pd.DataFrame(columns = ['from', 'to', 'weight']) \n",
    "\n",
    "    for ap in columns:\n",
    "        # para cada AP me quedo con las instancias donde el RSSI esta en el rango\n",
    "        # (max-th) intentando estimar las instancias mas cercanas al AP\n",
    "        max_val = df_data_train[ap].max()\n",
    "        df_aux_i = df_data_train[df_data_train[ap]  > (max_val - th)]\n",
    "        df_aux_i = df_aux_i.drop(ap, axis=1) \n",
    "        df_aux_i.head()\n",
    "\n",
    "        for k, v in df_aux_i.mean().items():\n",
    "            # armo las aristas con el promedio de RSSI que ven las instancias \n",
    "            # filtradas al resto de los APs\n",
    "            # weight = v\n",
    "            # if df_G.loc[(df_G['from'] == k) & (df_G['to'] == ap)].weight.any():\n",
    "            #     weight = np.mean([float(df_G.loc[(df_G['from'] == k) & (df_G['to'] == ap)].weight), weight])\n",
    "            #     df_G.loc[(df_G['from'] == k) & (df_G['to'] == ap)] = k, ap, weight\n",
    "            df_G = df_G.append({'from':ap, 'to': k, 'weight': v}, ignore_index=True)\n",
    "        \n",
    "\n",
    "    edge_index_first_row = []\n",
    "    edge_index_second_row = []\n",
    "    edge_attr = []\n",
    "    for index, row in df_G.iterrows():\n",
    "        edge_index_first_row.append(columns.index(row['from']))\n",
    "        edge_index_second_row.append(columns.index(row['to']))\n",
    "        edge_attr.append([float(row.weight)])\n",
    "\n",
    "    \n",
    "    edge_index = torch.tensor([edge_index_first_row, edge_index_second_row], dtype=torch.long)\n",
    "    edge_attr = torch.tensor(edge_attr, dtype=torch.float)                           \n",
    "    edge_index, edge_attr = to_undirected(edge_index, edge_attr, reduce=\"mean\")\n",
    "    return edge_index, edge_attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(X, y, graph):\n",
    "    dataset = []\n",
    "    for i in range(len(y)):\n",
    "        data = deepcopy(graph)\n",
    "        data.x = torch.Tensor(X[i])\n",
    "        data.y = torch.Tensor(y[i])\n",
    "        data.train_mask = torch.Tensor([True]*len(y))\n",
    "        data.val_mask = torch.Tensor([True]*len(y))\n",
    "        data.test_mask = torch.Tensor([True]*len(y))                \n",
    "        dataset.append(data)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN_GCNConv(torch.nn.Module):\n",
    "    def __init__(self, conv_out_features: list = [16, 20]):\n",
    "        super().__init__()\n",
    "        self.conv_out_features = conv_out_features\n",
    "        self.conv1 = GCNConv(2, self.conv_out_features[0], bias=True, normalize=True)\n",
    "        self.conv2 = GCNConv(self.conv_out_features[0], self.conv_out_features[1], bias=True, normalize=True)\n",
    "        self.fc = torch.nn.Linear(self.conv_out_features[-1]*15,16)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        # print(\"After Conv1: \", x.shape)\n",
    "\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)        \n",
    "        # print(\"After Conv2: \", x.shape)\n",
    "\n",
    "        # x = torch.flatten(x, 0)\n",
    "        x = torch.reshape(x, (int(x.shape[0]/15),self.conv_out_features[-1]*15))\n",
    "        # print(\"After Flatten: \", x.shape)\n",
    "        \n",
    "        x = self.fc(x)\n",
    "        x = F.relu(x)        \n",
    "        # print(\"After FC: \", x.shape)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN_TAGConv(torch.nn.Module):\n",
    "    def __init__(self, k: list = [1,1], conv_out_features: list = [16, 20]):\n",
    "        super().__init__()\n",
    "        self.k = k\n",
    "        self.conv_out_features = conv_out_features\n",
    "        self.conv1 = TAGConv(2, self.conv_out_features[0], K=self.k[0], bias=True, normalize=True)\n",
    "        self.conv2 = TAGConv(self.conv_out_features[0], self.conv_out_features[1], K=self.k[1], bias=True, normalize=True)\n",
    "        self.fc = torch.nn.Linear(self.conv_out_features[-1]*15,16)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        # print(\"After Conv1: \", x.shape)\n",
    "\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)        \n",
    "        # print(\"After Conv2: \", x.shape)\n",
    "\n",
    "        # x = torch.flatten(x, 0)\n",
    "        x = torch.reshape(x, (int(x.shape[0]/15),self.conv_out_features[-1]*15))\n",
    "        # print(\"After Flatten: \", x.shape)\n",
    "        \n",
    "        x = self.fc(x)\n",
    "        x = F.relu(x)        \n",
    "        # print(\"After FC: \", x.shape)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN_GraphConv(torch.nn.Module):\n",
    "    def __init__(self, aggr = \"add\", conv_out_features: list = [16, 20]):\n",
    "        super().__init__()\n",
    "        self.aggr = aggr\n",
    "        self.conv_out_features = conv_out_features\n",
    "        self.conv1 = GraphConv(2, self.conv_out_features[0], aggr=self.aggr, bias=True)\n",
    "        self.conv2 = GraphConv(self.conv_out_features[0], self.conv_out_features[1], aggr=self.aggr, bias=True)\n",
    "        self.fc = torch.nn.Linear(self.conv_out_features[-1]*15,16)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        # print(\"After Conv1: \", x.shape)\n",
    "\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)        \n",
    "        # print(\"After Conv2: \", x.shape)\n",
    "\n",
    "        # x = torch.flatten(x, 0)\n",
    "        x = torch.reshape(x, (int(x.shape[0]/15),self.conv_out_features[-1]*15))\n",
    "        # print(\"After Flatten: \", x.shape)\n",
    "        \n",
    "        x = self.fc(x)\n",
    "        # x = F.relu(x)        \n",
    "        # print(\"After FC: \", x.shape)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YHsZyzkAZKlK",
    "tags": []
   },
   "source": [
    "## Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(data.edge_attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.edge_attr = (data.edge_attr - data.edge_attr.mean()) / data.edge_attr.std()\n",
    "data.edge_attr = (data.edge_attr - data.edge_attr.min()) / data.edge_attr.max()\n",
    "data.edge_attr *= 4\n",
    "# data.edge_attr = torch.nn.functional.normalize(data.edge_attr, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = to_networkx(data, edge_attrs=[\"edge_attr\"], to_undirected=True)\n",
    "weights = nx.get_edge_attributes(g,'edge_attr').values()\n",
    "# pos = nx.circular_layout(g)\n",
    "graph = nx.draw(g, width=list(weights), with_labels=True, node_color='black', font_color=\"white\", font_family='Helvetica')\n",
    "plt.savefig(\"mnav_graph.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 356,
     "status": "ok",
     "timestamp": 1635462144424,
     "user": {
      "displayName": "Facundo Lezama",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjvX7z6H8HKQ9Q4HSRF20NaPjvwtJ3pQh0TGed2XQ=s64",
      "userId": "08104556030349101598"
     },
     "user_tz": 180
    },
    "id": "0MsSCtZkUDbj"
   },
   "outputs": [],
   "source": [
    "# Split de los datos y armado del objeto Data con el grafo\n",
    "\n",
    "X_train, X_test, y_train, y_test = preprocess_dataset(dataset)\n",
    "edge_index, edge_attr = graph_creator(X_train[:,:15], th=10) #el grafo lo armo solo con los datos de 2.4Ghz\n",
    "data = Data(edge_index=edge_index, edge_attr=edge_attr, num_nodes=15)\n",
    "print(f\"Undirected: {data.is_undirected()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Armado del dataset\n",
    "\n",
    "x_training_data = np.reshape(X_train,(X_train.shape[0],15,2))\n",
    "x_test_data = np.reshape(X_test,(X_test.shape[0],15,2))\n",
    "y_training_data = y_train\n",
    "y_test_data = y_test\n",
    "\n",
    "#normalize (x-mean)/std\n",
    "mean = x_training_data.mean(axis=0)\n",
    "std = x_training_data.std(axis=0)\n",
    "\n",
    "x_training_data = x_training_data - mean\n",
    "x_training_data /= std\n",
    "x_test_data = x_test_data - mean\n",
    "x_test_data /= std\n",
    "\n",
    "train_dataset = build_dataset(x_training_data, y_training_data, data)\n",
    "test_dataset = build_dataset(x_test_data, y_test_data, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### GCNConv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GNN_GCNConv(conv_out_features=[20,20]).to(device)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=len(test_dataset), shuffle=False)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=5e-4)\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = []\n",
    "train_accuracy = []\n",
    "test_loss = []\n",
    "test_accuracy = []\n",
    "\n",
    "m = torch.nn.Softmax(dim=1)\n",
    "\n",
    "for epoch in range(100):\n",
    "    print(f\"Epoch: {epoch}\")\n",
    "    \n",
    "    # TRAIN\n",
    "    model.train()\n",
    "    train_accuracy_epoch = []\n",
    "    train_loss_epoch = []\n",
    "    for data in train_loader:\n",
    "        import ipdb; ipdb.set_trace()\n",
    "        optimizer.zero_grad()\n",
    "        # print(data.x.shape)\n",
    "        # print(data.y.shape)\n",
    "        \n",
    "        \n",
    "        out = model(data.to(device))\n",
    "        # print(out)\n",
    "        # out_softmax = np.array(torch.argmax(out, dim=0)).item()\n",
    "        # out_softmax = torch.tensor([out_softmax])\n",
    "        loss_result = loss(out, data.y.type(torch.long))        \n",
    "        loss_result.backward()\n",
    "        \n",
    "        train_loss_epoch.append(loss_result.detach().cpu())\n",
    "        output = m(out)\n",
    "        train_accuracy_epoch.append(accuracy_score(data.y.cpu().reshape(-1).type(torch.long), np.array(torch.argmax(output.cpu(), axis=1))))\n",
    "\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "    # if scheduler.get_last_lr()[0] > 0.0005:\n",
    "    #     scheduler.step()\n",
    "\n",
    "    train_accuracy.append(np.mean(train_accuracy_epoch))\n",
    "    train_loss.append(np.mean(train_loss_epoch))\n",
    "\n",
    "    # VALIDATION\n",
    "    model.eval()\n",
    "    test_accuracy_epoch = []\n",
    "    test_loss_epoch = []\n",
    "    for data in test_loader:\n",
    "        out = model(data.to(device))\n",
    "        loss_result = loss(out, data.y.type(torch.long))        \n",
    "        \n",
    "        test_loss_epoch.append(loss_result.detach().cpu())\n",
    "        output = m(out)\n",
    "        test_accuracy_epoch.append(accuracy_score(data.y.cpu().reshape(-1).type(torch.long), np.array(torch.argmax(output.cpu(), axis=1))))\n",
    "\n",
    "    test_accuracy.append(np.mean(test_accuracy_epoch))\n",
    "    test_loss.append(np.mean(test_loss_epoch))\n",
    "\n",
    "\n",
    "print(f\"Last LR: {scheduler.get_last_lr()}\")\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(train_loss, label=\"Train loss\")\n",
    "plt.plot(test_loss, label=\"Validation loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(train_accuracy, label=\"Train accuracy\")\n",
    "plt.plot(test_accuracy, label=\"Validation accuracy\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = torch.nn.Softmax(dim=1)\n",
    "output = m(out)\n",
    "accuracy = accuracy_score(data.y.cpu().reshape(-1).type(torch.long), np.array(torch.argmax(output.cpu(), axis=1)))\n",
    "\n",
    "print(accuracy)\n",
    "print(classification_report(data.y.cpu().reshape(-1).type(torch.long), np.array(torch.argmax(output.cpu(), axis=1))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### TAGConv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GNN_TAGConv(k=[2,2], conv_out_features=[20,20]).to(device)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=len(test_dataset), shuffle=False)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=5e-4)\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = []\n",
    "train_accuracy = []\n",
    "test_loss = []\n",
    "test_accuracy = []\n",
    "best_test_accuracy = 0\n",
    "\n",
    "m = torch.nn.Softmax(dim=1)\n",
    "\n",
    "for epoch in range(50):\n",
    "    print(f\"Epoch: {epoch}\")\n",
    "    \n",
    "    # TRAIN\n",
    "    model.train()\n",
    "    train_accuracy_epoch = []\n",
    "    train_loss_epoch = []\n",
    "    for data in train_loader:\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        # print(data.x.shape)\n",
    "        # print(data.y.shape)\n",
    "        \n",
    "        \n",
    "        out = model(data.to(device))\n",
    "        # print(out)\n",
    "        # out_softmax = np.array(torch.argmax(out, dim=0)).item()\n",
    "        # out_softmax = torch.tensor([out_softmax])\n",
    "        loss_result = loss(out, data.y.type(torch.long))        \n",
    "        loss_result.backward()\n",
    "        \n",
    "        train_loss_epoch.append(loss_result.detach().cpu())\n",
    "        output = m(out)\n",
    "        train_accuracy_epoch.append(accuracy_score(data.y.cpu().reshape(-1).type(torch.long), np.array(torch.argmax(output.cpu(), axis=1))))\n",
    "\n",
    "\n",
    "        optimizer.step()\n",
    "    \n",
    "    if (epoch+1)%10 == 0:\n",
    "        scheduler.step()\n",
    "\n",
    "    train_accuracy.append(np.mean(train_accuracy_epoch))\n",
    "    train_loss.append(np.mean(train_loss_epoch))\n",
    "\n",
    "    # VALIDATION\n",
    "    model.eval()\n",
    "    test_accuracy_epoch = []\n",
    "    test_loss_epoch = []\n",
    "    for data in test_loader:\n",
    "        out = model(data.to(device))\n",
    "        loss_result = loss(out, data.y.type(torch.long))        \n",
    "        \n",
    "        test_loss_epoch.append(loss_result.detach().cpu())\n",
    "        output = m(out)\n",
    "        test_accuracy_epoch.append(accuracy_score(data.y.cpu().reshape(-1).type(torch.long), np.array(torch.argmax(output.cpu(), axis=1))))\n",
    "\n",
    "    test_accuracy.append(np.mean(test_accuracy_epoch))\n",
    "    if test_accuracy[-1] > best_test_accuracy:\n",
    "        best_test_accuracy = test_accuracy[-1]        \n",
    "        torch.save(model.state_dict(), \"MNAV_best_model.pth\")\n",
    "        \n",
    "    test_loss.append(np.mean(test_loss_epoch))\n",
    "    print(f\"    Train Loss {np.mean(train_loss_epoch)}, Val Loss {np.mean(test_loss_epoch)}\")\n",
    "\n",
    "\n",
    "print(f\"Last LR: {scheduler.get_last_lr()}\")\n",
    "print(f\"Best Accuracy: Train {np.max(train_accuracy)}, Val {np.max(test_accuracy)}\")\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(train_loss, label=\"Train loss\")\n",
    "plt.plot(test_loss, label=\"Validation loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(train_accuracy, label=\"Train accuracy\")\n",
    "plt.plot(test_accuracy, label=\"Validation accuracy\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = torch.nn.Softmax(dim=1)\n",
    "output = m(out)\n",
    "accuracy = accuracy_score(data.y.cpu().reshape(-1).type(torch.long), np.array(torch.argmax(output.cpu(), axis=1)))\n",
    "\n",
    "print(accuracy)\n",
    "print(classification_report(data.y.cpu().reshape(-1).type(torch.long), np.array(torch.argmax(output.cpu(), axis=1))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### GraphConv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GNN_GraphConv(aggr=\"mean\", conv_out_features=[20,20]).to(device)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=len(test_dataset), shuffle=False)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=5e-4)\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = []\n",
    "train_accuracy = []\n",
    "test_loss = []\n",
    "test_accuracy = []\n",
    "\n",
    "m = torch.nn.Softmax(dim=1)\n",
    "\n",
    "for epoch in range(100):\n",
    "    print(f\"Epoch: {epoch}\")\n",
    "    \n",
    "    # TRAIN\n",
    "    model.train()\n",
    "    train_accuracy_epoch = []\n",
    "    train_loss_epoch = []\n",
    "    for data in train_loader:\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        # print(data.x.shape)\n",
    "        # print(data.y.shape)\n",
    "        \n",
    "        \n",
    "        out = model(data.to(device))\n",
    "        # print(out)\n",
    "        # out_softmax = np.array(torch.argmax(out, dim=0)).item()\n",
    "        # out_softmax = torch.tensor([out_softmax])\n",
    "        \n",
    "        import ipdb; ipdb.set_trace()\n",
    "        \n",
    "        loss_result = loss(out, data.y.type(torch.long))        \n",
    "        loss_result.backward()\n",
    "        \n",
    "        train_loss_epoch.append(loss_result.detach().cpu())\n",
    "        output = m(out)\n",
    "        train_accuracy_epoch.append(accuracy_score(data.y.cpu().reshape(-1).type(torch.long), np.array(torch.argmax(output.cpu(), axis=1))))\n",
    "\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "    # if scheduler.get_last_lr()[0] > 0.0005:\n",
    "    #     scheduler.step()\n",
    "\n",
    "    train_accuracy.append(np.mean(train_accuracy_epoch))\n",
    "    train_loss.append(np.mean(train_loss_epoch))\n",
    "\n",
    "    # VALIDATION\n",
    "    model.eval()\n",
    "    test_accuracy_epoch = []\n",
    "    test_loss_epoch = []\n",
    "    for data in test_loader:\n",
    "        out = model(data.to(device))\n",
    "        loss_result = loss(out, data.y.type(torch.long))        \n",
    "        \n",
    "        test_loss_epoch.append(loss_result.detach().cpu())\n",
    "        output = m(out)\n",
    "        test_accuracy_epoch.append(accuracy_score(data.y.cpu().reshape(-1).type(torch.long), np.array(torch.argmax(output.cpu(), axis=1))))\n",
    "\n",
    "    test_accuracy.append(np.mean(test_accuracy_epoch))\n",
    "    test_loss.append(np.mean(test_loss_epoch))\n",
    "\n",
    "\n",
    "print(f\"Last LR: {scheduler.get_last_lr()}\")\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(train_loss, label=\"Train loss\")\n",
    "plt.plot(test_loss, label=\"Validation loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(train_accuracy, label=\"Train accuracy\")\n",
    "plt.plot(test_accuracy, label=\"Validation accuracy\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = torch.nn.Softmax(dim=1)\n",
    "output = m(out)\n",
    "accuracy = accuracy_score(data.y.cpu().reshape(-1).type(torch.long), np.array(torch.argmax(output.cpu(), axis=1)))\n",
    "\n",
    "print(accuracy)\n",
    "print(classification_report(data.y.cpu().reshape(-1).type(torch.long), np.array(torch.argmax(output.cpu(), axis=1))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Análisis variando cantidad de muestras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('cuda:1')\n",
    "batch_size = 16\n",
    "learning_rate = 0.01  \n",
    "print_every = 5\n",
    "\n",
    "porcentajes = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
    "accuracy = {\"0.1\":[], \"0.2\":[], \"0.3\":[], \"0.4\":[], \"0.5\":[], \"0.6\":[], \"0.7\":[], \"0.8\":[], \"0.9\":[], \"1\":[]}\n",
    "\n",
    "for porc in porcentajes:\n",
    "    print('Porcentaje de datos: ', porc)\n",
    "    \n",
    "    for i in range(10):    \n",
    "\n",
    "        X_train, X_test, y_train, y_test = preprocess_dataset(dataset, dataset_percentage=porc)\n",
    "\n",
    "        # x_training_data = np.reshape(X_train,(X_train.shape[0],15,2))\n",
    "        # x_test_data = np.reshape(X_test,(X_test.shape[0],15,2))\n",
    "        y_training_data = y_train\n",
    "        y_test_data = y_test\n",
    "\n",
    "        #normalize (x-mean)/std\n",
    "        mean = X_train.mean(axis=0)\n",
    "        std = X_train.std(axis=0)\n",
    "\n",
    "        x_training_data = X_train - mean\n",
    "        x_training_data /= std\n",
    "        x_test_data = X_test - mean\n",
    "        x_test_data /= std\n",
    "\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        from sklearn.metrics import accuracy_score, classification_report     \n",
    "        \n",
    "        neigh = KNeighborsClassifier(n_neighbors=3)\n",
    "        neigh.fit(x_training_data, y_training_data)\n",
    "        y_pred_knn = neigh.predict(x_test_data)\n",
    "        acc = accuracy_score(y_test_data, y_pred_knn)\n",
    "\n",
    "        print(f\"Accuracy: {acc}\")\n",
    "        accuracy[str(porc)].append(acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "n4qV8TIQYl62",
    "gfqVFAK32Th4",
    "fmC2Dsazdx7i",
    "YHsZyzkAZKlK",
    "gU46OaNwNj7Z",
    "P3QFKooiTRQF",
    "DdJpdEQmWRuO",
    "9Y6Zs61LXiik",
    "MTSumaDohl3K",
    "d2MElifG5G5q",
    "2EjKrCuHim66",
    "iSVeCEtNim7C",
    "1uwrgRzJim7D",
    "kpCBGsGe6J3Z",
    "Tb5Kdlky-4DN",
    "2cJTvncIOZ6I",
    "SwovNzD_THq_",
    "V5fnHhVyXMC4"
   ],
   "name": "tesis_mnav_simple_gnn.ipynb",
   "provenance": [
    {
     "file_id": "1AOWB0JuB1YtTUv8waFAYjdVb27Oi_zVw",
     "timestamp": 1617051239005
    },
    {
     "file_id": "1-veytfLbAuWzFrDTNxXhvsEfn8Th0D2K",
     "timestamp": 1616972473551
    },
    {
     "file_id": "1iQI9YVQL7SxQAvy2A4VAE8jJCv7nBfbI",
     "timestamp": 1615845377065
    }
   ]
  },
  "interpreter": {
   "hash": "feb5c0b0a29f8cb9ce82bbdb934224dc5d3b107b617c193c25692c16f2b91aa2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
